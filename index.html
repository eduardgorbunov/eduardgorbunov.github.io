<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v5.2.0, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v5.2.0, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/soccer-ball-128x128.png" type="image/x-icon">
  <meta name="description" content="">
  
  
  <title>Eduard Gorbunov</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
  
</head>
<body>
  
  <section class="menu cid-qTkzRZLJNu" once="menu" id="menu1-0">

    

    <nav class="navbar navbar-expand beta-menu navbar-dropdown align-items-center navbar-fixed-top navbar-toggleable-sm">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
                <span></span>
            </div>
        </button>
        <div class="menu-logo">
            <div class="navbar-brand">
                
                <span class="navbar-caption-wrap"><a class="navbar-caption text-white display-2" href="index.html">Eduard Gorbunov</a></span>
            </div>
        </div>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item">
                    <a class="nav-link link text-white display-4" href="index.html">
                        
                        Home<br></a>
                </li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#header5-16">Skills and Interests<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#timeline1-f">
                        
                        Education<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#timeline1-2a">
                        
                        Work Experience<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#header5-v">
                        
                        News<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="publications.html">
                        
                        Publications<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="conferences.html#header5-s">Talks<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="conferences.html#header5-t">Posters<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="teaching.html">Teaching<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="page6.html">Books<br></a></li></ul>
            
        </div>
    </nav>
</section>

<section class="header3 cid-qYP1dtKorN" id="header3-a">

    

    

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-figure" style="width: 30%;">
                <img src="assets/images/picture-188x334.jpg" alt="Mobirise" title="">
            </div>

            <div class="media-content">
                <h1 class="mbr-section-title mbr-white pb-3 mbr-fonts-style display-1">
                    Biography
                </h1>
                
                <div class="mbr-section-text mbr-white pb-3 ">
                    <p class="mbr-text mbr-fonts-style display-5">Currently I am a junior researcher at <a href="https://mipt.ru/english/" class="text-primary" target="_blank">MIPT</a>. I obtained my PhD degree at&nbsp;<a href="https://mipt.ru/english/" target="_blank">Moscow Institute of Physics and Technology</a>, <a href="https://mipt.ru/english/edu/phystechschools/psami" target="_blank">Phystech School of Applied Mathematics and Informatics</a>&nbsp;where&nbsp;I worked under the supervision of professors&nbsp;<a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-primary">Alexander Gasnikov</a>&nbsp;and&nbsp;<a href="https://richtarik.org/index.html" class="text-primary" target="_blank">Peter Richtárik</a>. My research interests include Stochastic Optimization and its applications to Machine Learning, Distributed Optimization, Derivative-Free Optimization, and Variational Inequalities. I am also a co-organizer of <a href="http://www.mathnet.ru/php/conference.phtml?option_lang=rus&eventID=31&confid=1794" target="_blank">Russian Optimization Seminar</a> and&nbsp;<a href="https://optmipt.github.io/" target="_blank">the research seminar on Optimization at MIPT</a>.<br><strong>Selected awards: </strong><br>-<strong>&nbsp;</strong><a href="https://yandex.ru/scholarships" target="_blank" class="text-primary">The Ilya Segalovich Award&nbsp;2019</a>&nbsp;(highly selective)<br>-&nbsp;<a href="https://vk.com/wall-44001716_4739" target="_blank">Huawei scholarship for bachelor and master students at MIPT</a><br><br><a href="https://www.researchgate.net/profile/Eduard_Gorbunov" target="_blank">ResearchGate</a><br><a href="https://arxiv.org/search/math?searchtype=author&query=Gorbunov%2C+E" target="_blank">arXiv</a><br> <a href="https://scholar.google.ru/citations?hl=en&authuser=2&user=QPVriwoAAAAJ" target="_blank" class="text-primary">Google scholar</a><br><a href="https://twitter.com/ed_gorbunov" target="_blank">Twitter</a><br><a href="assets/files/CV.pdf" target="_blank" class="text-primary">Download my CV</a><br>My main e-mail: eduard.gorbunov at phystech dot edu<br>My second e-mail: ed-gorbunov at yandex dot ru<a href="assets/files/CV.pdf" target="_blank"><br></a><br>
                    </p>
                </div>
                
            </div>
        </div>
    </div>

</section>

<section class="header5 cid-qYQDzdxwR8" id="header5-16">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1">
                    Skills and Interests</h1>
                
                
            </div>
        </div>
    </div>

    
</section>

<section class="mbr-section article content12 cid-qYQEjWGZkg" id="content12-18">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><font color="#000000"><strong>Computer skills:</strong></font></div><ul>
                    <li>Operating systems: Microsoft Windows, Linux, Mac OS</li><li>Programming languages: Python, LaTeX, C, C++<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-qYQFYKn1lc" id="content12-19">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><strong>Languages:</strong></div><ul>
                    <li>Russian (native)</li><li>English (advanced)</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-qYQGkPjvpj" id="content12-1a">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><font color="#000000"><strong>Interests:</strong></font></div><ul>
                    <li>Football: 9 years in football school in Rybinsk, Russia. Now I am playing for an&nbsp;<a href="http://www.lfl.ru/person86180" target="_blank" class="text-secondary">amateur team</a>.</li><li>Table tennis, fitness</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="timeline1 cid-qYP2SIf7dk" id="timeline1-f">

    

    

    <div class="container align-center">
        <h2 class="mbr-section-title pb-3 mbr-fonts-style display-1">
            Education</h2>
        

        <div class="container timelines-container" mbri-timelines="">
            <div class="row timeline-element reverse separline">      
                 <div class="timeline-date-panel col-xs-12 col-md-6  align-left">         
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">Sep 2014 - Jul 2018</p>
                    </div>
                </div>
           <span class="iconBackground"></span>
            <div class="col-xs-12 col-md-6 align-right">
                <div class="timeline-text-content">
                    <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">BSc in Applied Mathematics Moscow Institute of Physics and Technology</h4>
                    <p class="mbr-timeline-text mbr-fonts-style display-7">Department of Control and Applied Mathematics<br>Thesis: <a href="assets/files/gorbunov_diplom.pdf" target="_blank" class="text-secondary">Accelerated Directional Searchs and Gradient-Free Methods with non-Euclidean prox-structure</a><br>Advisor: <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a></p>
                 </div>
            </div>
            </div>

            <div class="row timeline-element separline">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Sep 2018 - Jul 2020</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">MSc in Applied Mathematics Moscow Institute of Physics and Technology</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Phystech School of Applied Mathematics and Informatics<br>Thesis:&nbsp;<a href="assets/files/gorbunov_ms_thesis.pdf" target="_blank" class="text-secondary">Derivative-free and stochastic optimization methods, decentralized distributed optimization</a><br>Advisor: <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a></p>
                    </div>
                </div>
            </div> 


            <div class="row timeline-element reverse">
                <div class="timeline-date-panel col-xs-12 col-md-6  align-left">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Sep 2020 - Dec 2021</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-right">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">PhD in Computer Science<br>&nbsp;Moscow Institute of Physics and Technology</h4>      
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Phystech School of Applied Mathematics and Informatics<br>&nbsp;Thesis:&nbsp;<a href="https://arxiv.org/abs/2112.10645" class="text-secondary" target="_blank">Distributed and Stochastic Optimization Methods with Gradient Compression and Local Steps</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Advisors: <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>&nbsp;and <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a><br>Links: <a href="assets/files/PhD_defense.pdf" class="text-secondary text-primary" target="_blank">slides</a> and <a href="https://youtu.be/HKVp2oNedi4" class="text-secondary" target="_blank">video</a> of the defense</p>
                    </div>
                </div>
            </div>

            


            


            

            

            

            

            

            

            
        </div>
    </div>
</section>

<section class="timeline1 cid-rSyhxSvbwJ" id="timeline1-2a">

    

    

    <div class="container align-center">
        <h2 class="mbr-section-title pb-3 mbr-fonts-style display-1">
            Work Experience</h2>
        

        <div class="container timelines-container" mbri-timelines="">
            <div class="row timeline-element reverse separline">      
                 <div class="timeline-date-panel col-xs-12 col-md-6  align-left">         
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">Aug 2017 - Oct 2019</p>
                    </div>
                </div>
           <span class="iconBackground"></span>
            <div class="col-xs-12 col-md-6 align-right">
                <div class="timeline-text-content">
                    <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Moscow Institute of Physics and Technology</h4>
                    <p class="mbr-timeline-text mbr-fonts-style display-7">Researcher at <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>'s group</p>
                 </div>
            </div>
            </div>

            <div class="row timeline-element separline">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Feb 2018 - Jun 2019</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Moscow Institute of Physics and Technology</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7"><a href="teaching.html" class="text-secondary">Teaching assistant</a>&nbsp;for the following courses: Algorithms and Models of Computation, Probability Theory</p>
                    </div>
                </div>
            </div> 


            <div class="row timeline-element reverse separline">
                <div class="timeline-date-panel col-xs-12 col-md-6  align-left">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            May 2019 - Aug 2019</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-right">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">
                            Huawei, Moscow</h4>      
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Intern in Media Laboratory (research, C++ programming)&nbsp;</p>
                    </div>
                </div>
            </div>

            <div class="row timeline-element separline">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Aug 2019 - Jul 2020</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">
                            Huawei-MIPT group, Moscow</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Researcher</p>
                    </div>
                </div>
            </div>


            <div class="row timeline-element reverse separline">
                <div class="timeline-date-panel col-xs-12 col-md-6  align-left">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Nov 2019 - Apr 2020</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-right">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Institute for Information Transmission Problems of Russian Academy of Sciences, Moscow</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Junior researcher at Department of Mathematical Methods of Predictive Modelling</p>
                    </div>
                </div>
            </div>


            <div class="row timeline-element separline">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                       <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Feb 2020 - Dec 2020</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Moscow Institute of Physics and Technology</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Junior researcher at Laboratory of Numerical Methods of Applied Structural Optimization
                        </p>
                    </div>
                </div>
            </div>

            <div class="row timeline-element reverse separline">
                <div class="timeline-date-panel col-xs-12 col-md-6  align-left">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                           Feb 2020 - Dec 2020</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-right">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Russian Presidential Academy of National Economy and Public Administration, Moscow</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Junior researcher at Joint Research Laboratory of Applied Mathematics, <a href="https://www.ranepa.ru/eng/" target="_blank" class="text-secondary">RANEPA</a>-<a href="https://mipt.ru/english/" target="_blank" class="text-secondary">MIPT</a></p>
                    </div>
                </div>
            </div>

            <div class="row timeline-element separline">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                           May 2020 - Now</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Moscow Institute of Physics and Technology</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">Junior researcher at Laboratory of Advanced Combinatorics and Network Applications</p>
                    </div>
                </div>
            </div>

            <div class="row timeline-element reverse separline">
                <div class="timeline-date-panel col-xs-12 col-md-6  align-left">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                           May 2020 - Dec 2021</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-right">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">National Research University Higher School of Economics</h4>

                        <p class="mbr-timeline-text mbr-fonts-style display-7">Research Assistant at&nbsp;International Laboratory of Stochastic Algorithms and High-Dimensional Inference<br></p>
                    </div>
                </div>
            </div>

            <div class="row timeline-element separline">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                       <p class="mbr-timeline-date mbr-fonts-style display-5">
                          Sep 2020 - January 2022</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">
                            Yandex.Research-MIPT Lab</h4> 
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Junior researcher at the joint Lab of Yandex.Research and Moscow Institute of Physics and Technology</p>
                    </div>
                </div>
            </div>

            <div class="row timeline-element reverse">
                <div class="timeline-date-panel col-xs-12 col-md-6  align-left">
                    <div class="time-line-date-content">
                       <p class="mbr-timeline-date mbr-fonts-style display-5">
                          Feb 2022 - May 2022</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-right">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">Mila</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Research consultant at Mila, Montreal<br>Group of <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a></p>
                    </div>
                </div>
            </div>

            
        </div>
    </div>
</section>

<section class="header5 cid-qYPQ5TK196" id="header5-v">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1">
                    News</h1>
                
                
            </div>
        </div>
    </div>

    
</section>

<section class="mbr-section article content12 cid-sQ5CaGrZHg" id="content12-32">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><strong>[13 July, 2022]
<br>Outstanding reviewer award (ICML 2022)!<br></strong>I am happy to share that I got <a href="https://drive.google.com/file/d/1jJ9jquw_6CBMpp7DYFAy7fbrDRD_SwlX/view?usp=sharing}" class="text-secondary" target="_blank">outstanding reviewer award at ICML 2022</a> and a free registration to the conference.<br>---------------------------------------------------------<br></li><li><span style="font-size: 1.5rem;"><strong>[3 July, 2022]
<br></strong></span><strong>Talk at<a href="http://motor2022.krc.karelia.ru/en/section/1" class="text-secondary" target="_blank"> MOTOR 2022</a>.&nbsp;<br></strong><span style="font-size: 1.5rem;">I presented <a href="https://arxiv.org/abs/2203.02383" class="text-secondary" target="_blank">"Distributed Methods with Absolute Compression and Error Compensation"</a> - joint work with <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a>.<br></span><span style="font-size: 1.5rem;">[<a href="assets/files/EC_SGD_abs_compression.pdf" class="text-secondary" target="_blank">slides</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[14 June, 2022]<br>New paper out: </strong><a href="https://arxiv.org/abs/2206.07021" class="text-secondary" target="_blank">"Federated Optimization Algorithms with Random Reshuffling and Gradient Compression"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=R-xZRIAAAAAJ&hl=en" class="text-secondary" target="_blank">Abdurakhmon Sadiev</a>, <a href="https://scholar.google.ru/citations?user=4w2W9KQAAAAJ&hl=en" class="text-secondary" target="_blank">Grigory Malinovsky</a>, <a href="https://scholar.google.ru/citations?hl=en&user=OBbPecwAAAAJ" class="text-secondary" target="_blank">Igor Sokolov</a>, <a href="https://www.akhaled.org/" class="text-secondary" target="_blank">Ahmed Khaled</a>,
&nbsp;</span><a href="https://burlachenkok.github.io/" class="text-secondary" target="_blank">Konstantin Burlachenko</a>, <a href="https://richtarik.org/" class="text-secondary" target="_blank">Peter Richtárik</a>.<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[2 June, 2022]
<br></strong></span><strong>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/abs/2206.01095" class="text-secondary" target="_blank">"Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise</a></span><a href="https://arxiv.org/abs/2206.01095" class="text-secondary" target="_blank">"</a> - joint work with <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a>, <a href="https://ca.linkedin.com/in/daviddobre" class="text-secondary" target="_blank">David Dobre</a>, <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=en" class="text-secondary" target="_blank">Pavel Dvurechensky</a>, <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Gasnikov</a>, <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[1 June, 2022]
<br></strong></span><strong>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/abs/2206.00529" class="text-secondary" target="_blank">"Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top"</a> - joint work with <a href="https://samuelhorvath.github.io/" class="text-secondary" target="_blank">Samuel Horváth</a>, <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>, <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.&nbsp;<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[17 May, 2022]<br></strong></span><strong>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/abs/2205.08446" class="text-secondary" target="_blank">"Last-Iterate Convergence of Optimistic Gradient Method for Monotone Variational Inequalities"</a></span><strong> </strong><span style="font-size: 1.5rem;">- joint work with <a href="https://adrientaylor.github.io///" class="text-secondary" target="_blank">Adrien Taylor</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.<br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[15 May, 2022]</strong><br><strong>Two papers accepted to ICML 2022:</strong><br><br>1. <a href="https://arxiv.org/abs/2106.11257" class="text-secondary" target="_blank">"Secure Distributed Training at Scale"</a> - joint work with <a href="https://scholar.google.ru/citations?user=HdwzsCMAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Borzunov</a>, <a href="https://scholar.google.ru/citations?user=LRKQhcYAAAAJ&hl=en" class="text-secondary" target="_blank">Michael Diskin</a>&nbsp;</span>and <a href="https://scholar.google.ru/citations?hl=en&user=930PERsAAAAJ" class="text-secondary" target="_blank">Max Ryabinin</a>.<br><br>2. <a href="https://arxiv.org/abs/2202.00998" class="text-secondary" target="_blank">"3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation"</a> - joint work with <a href="https://richtarik.org/" class="text-secondary" target="_blank">Peter Richtárik</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov" class="text-secondary" target="_blank">Igor Sokolov</a>, <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en" class="text-secondary" target="_blank">Ilyas Fatkhullin</a>, <a href="https://elnurgasanov.com/" class="text-secondary" target="_blank">Elnur Gasanov</a>, and <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>.<br><br>The list of accepted papers is available <a href="https://icml.cc/Conferences/2022/AcceptedPapersInitial" class="text-secondary" target="_blank">here</a>.<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[25 April, 2022]<br>Talk at <a href="assets/files/WS-FL_FLyer.pdf" class="text-secondary" target="_blank">Lagrange Workshop on Federated Learning</a>.<br></strong>I presented <a href="https://arxiv.org/abs/2106.11257" class="text-secondary" target="_blank">"Secure Distributed Training at Scale"</a> - joint work with <a href="https://scholar.google.ru/citations?user=HdwzsCMAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Borzunov</a>, <a href="https://scholar.google.ru/citations?user=LRKQhcYAAAAJ&hl=en" class="text-secondary" target="_blank">Michael Diskin</a> and<br><a href="https://scholar.google.ru/citations?hl=en&user=930PERsAAAAJ" class="text-secondary" target="_blank">Max Ryabinin</a>.<br></span>[<a href="https://drive.google.com/file/d/15QzjUEB1gsyCh637elzVhs9-NJqCbcqy/view" class="text-secondary" target="_blank">slides</a>]<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[8 April, 2022]<br>One paper accepted to MOTOR 2022: </strong><a href="https://arxiv.org/abs/2203.02383" class="text-secondary" target="_blank">"Distributed Methods with Absolute Compression and Error Compensation"</a> - joint work with <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a>.<br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[28-29 March, 2022]</strong><br><strong>I presented 2 papers at AISTATS 2022:</strong><br>1. <a href="https://arxiv.org/abs/2110.04261" class="text-secondary" target="_blank">Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities  and Connections With Cocoercivity</a> - joint work with <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.
 [<a href="assets/files/EG_Last_iter_poster.pdf" class="text-success" target="_blank">poster</a>] [<a href="https://virtual.aistats.org/virtual/2022/poster/3138" class="text-success" target="_blank">page on AISTATS website</a>]<br></span>2. <a href="https://arxiv.org/abs/2111.08611" class="text-secondary" target="_blank">Stochastic Extragradient: General Analysis and Improved Rates</a> - joint work with <a href="https://scholar.google.ru/citations?user=P5d_140AAAAJ&hl=en" class="text-secondary" target="_blank">Hugo Berard</a>, <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a> and <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a>. [<a href="assets/files/SEG_poster.pdf" class="text-success" target="_blank">poster</a>] [<a href="https://virtual.aistats.org/virtual/2022/poster/3481" class="text-success" target="_blank">page on AISTATS website</a>]<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[13 March, 2022]</strong><br><strong>Talk at </strong><a href="https://cemse.kaust.edu.sa/ai/aii-symp-2022" class="text-secondary" target="_blank" style="font-weight: bold;">Rising Stars in AI Symposium 2022</a><strong>:</strong><br>I presented our work <a href="https://arxiv.org/abs/2110.04261" class="text-secondary" target="_blank">"Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity"</a>&nbsp;(joint work with<br><a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>). For me it was the first offline conference since 2020.<br>[<a href="assets/files/Last_iter_EG_Rising_stars_KAUST_2022.pdf" class="text-secondary" target="_blank">slides</a>] [<a href="https://webcast.kaust.edu.sa/Mediasite/Showcase/default/Presentation/88f38b4f0446440099e4f6b863d332d11d" class="text-success" target="_blank">video</a>]<br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[4 March, 2022]<br>New paper out: </strong><a href="https://arxiv.org/abs/2203.02383" class="text-secondary" target="_blank">"Distributed Methods with Absolute Compression and Error Compensation"</a>&nbsp;- joint work with <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a>.<br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[16 February, 2022]</strong><br><strong>Talk at Vector Institute: </strong>together with Max Ryabinin we presented our NeurIPS 2022 paper&nbsp;<a href="https://papers.nips.cc/paper/2021/hash/97275a23ca44226c9964043c8462be96-Abstract.html" class="text-secondary" target="_blank">"Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</a> - joint work with Vsevolod Plokhotnyuk and
&nbsp;</span><a href="http://www.cs.toronto.edu/~pekhimenko/" class="text-secondary" target="_blank">Gennady Pekhimenko</a>.<br>[<a href="assets/files/moshpit-pres-vector.pdf" class="text-secondary" target="_blank">slides</a>]<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[15 February, 2022]<br>New paper out: </strong><a href="https://arxiv.org/abs/2202.07262" class="text-secondary" target="_blank">"Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods"</a>&nbsp;- joint work with <a href="https://anbeznosikov.github.io/" class="text-secondary" target="_blank">Aleksandr Beznosikov</a>, <a href="https://scholar.google.ru/citations?user=P5d_140AAAAJ&hl=en" class="text-secondary" target="_blank">Hugo Berard</a>, <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a><br>---------------------------------------------------------
</span></li><li><span style="font-size: 1.5rem;"><strong>[2 February, 2022]<br>New paper out: </strong><a href="https://arxiv.org/abs/2202.00998" class="text-secondary" target="_blank">"3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation"</a>&nbsp;- joint work with <a href="https://richtarik.org/" class="text-secondary" target="_blank">Peter Richtárik</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov" class="text-secondary" target="_blank">Igor Sokolov</a>, <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en" class="text-secondary" target="_blank">Ilyas Fatkhullin</a>, <a href="https://elnurgasanov.com/" class="text-secondary" target="_blank">Elnur Gasanov</a>, and <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>.<br>---------------------------------------------------------
</span></li><li><span style="font-size: 1.5rem;"><strong>[18 January, 2022]</strong><br><strong>2 papers accepted to AISTATS 2022:</strong><br><br>1. <a href="https://arxiv.org/abs/2110.04261" class="text-secondary" target="_blank">Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities
&nbsp;</a></span><a href="https://arxiv.org/abs/2110.04261" class="text-secondary" target="_blank">and Connections With Cocoercivity</a>&nbsp;- joint work with <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.<br><br>2. <a href="https://arxiv.org/abs/2111.08611" class="text-secondary" target="_blank">Stochastic Extragradient: General Analysis and Improved Rates</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=P5d_140AAAAJ&hl=en" class="text-secondary" target="_blank">Hugo Berard</a>, <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a> and <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a>.<br><br>The list of accepted papers is available <a href="http://aistats.org/aistats2022/accepted.html" class="text-secondary" target="_blank">here</a>.<br>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[23 December, 2021]<br>PhD defense: </strong>I<strong>&nbsp;</strong>am happy to share that<strong>&nbsp;</strong>today <strong>I defended my dissertation</strong> "Distributed and Stochastic Optimization Methods with Gradient Compression and Local Steps" and received a degree of the candidate of physical and mathematical sciences (Russian equivalent of PhD).<br>[<a href="https://arxiv.org/abs/2112.10645" class="text-secondary" target="_blank">dissertation</a>] [<a href="assets/files/PhD_defense.pdf" class="text-secondary text-primary" target="_blank">slides of the defense</a>] [<a href="https://youtu.be/HKVp2oNedi4" class="text-success" target="_blank">video</a>]<br></span>---------------------------------------------------------</li><li><span style="font-size: 1.5rem;"><strong>[20 December, 2021]
<br></strong></span><span style="font-size: 1.5rem;">I gave a talk at&nbsp;<a href="https://www.epfl.ch/labs/mlo/" class="text-secondary" target="_blank">MLO EPFL</a> internal seminar about the paper <a href="https://papers.nips.cc/paper/2021/file/97275a23ca44226c9964043c8462be96-Supplemental.pdf" class="text-secondary" target="_blank">"Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</a> - joint work with <a href="https://scholar.google.ru/citations?user=930PERsAAAAJ&hl=en" class="text-secondary" target="_blank">Max Ryabinin</a>, Vsevolod Plokhotnyuk, and<br><a href="http://www.cs.toronto.edu/~pekhimenko/" class="text-secondary" target="_blank">Gennady Pekhimenko</a>.
<br></span><span style="font-size: 1.5rem;">[<a href="assets/files/Moshpit_SGD_MLO_EPFL_Dec_20_2021.pdf" class="text-secondary" target="_blank">slides</a>] [<a href="https://arxiv.org/abs/2103.03239" class="text-secondary text-primary" target="_blank">arXiv</a>] [<a href="https://papers.nips.cc/paper/2021/hash/97275a23ca44226c9964043c8462be96-Abstract.html" class="text-secondary" target="_blank">NeurIPS 2021 Proceedings</a>]&nbsp;<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[10 December, 2021]
<br></strong></span><span style="font-size: 1.5rem;"><strong>NeurIPS 2021 poster presentation: </strong>together with<br><a href="https://scholar.google.ru/citations?user=930PERsAAAAJ&hl=en" class="text-secondary" target="_blank">Max Ryabinin</a> we presented our paper <a href="https://papers.nips.cc/paper/2021/file/97275a23ca44226c9964043c8462be96-Supplemental.pdf" class="text-secondary" target="_blank">"Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</a>&nbsp;(joint work with Vsevolod Plokhotnyuk and <a href="http://www.cs.toronto.edu/~pekhimenko/" class="text-secondary" target="_blank">Gennady Pekhimenko</a>) at NeurIPS 2021 poster session. Although the conference was virtual, we had great discussions with several people.<br></span><span style="font-size: 1.5rem;">[<a href="assets/files/moshpit_poster.pdf" class="text-secondary" target="_blank">poster</a>] [<a href="https://arxiv.org/abs/2103.03239" class="text-secondary" target="_blank">arXiv</a>] [<a href="https://papers.nips.cc/paper/2021/hash/97275a23ca44226c9964043c8462be96-Abstract.html" class="text-secondary" target="_blank">NeurIPS 2021 Proceedings</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[1 December, 2021]</strong><br>I gave a talk at MTL MLOpt internal seminar about the recent paper <a href="https://arxiv.org/pdf/2110.04261.pdf" class="text-secondary" target="_blank">"Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity"</a> - joint work with <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.
<br></span><span style="font-size: 1.5rem;">[<a href="assets/files/Last_iter_EG_MTL_MLOpt_handout.pdf" class="text-secondary" target="_blank">slides</a>] [<a href="https://arxiv.org/abs/2110.04261" class="text-secondary text-primary" target="_blank">arXiv</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[17 November, 2021]</strong><br>I gave a talk at&nbsp;<a href="http://www.mathnet.ru/php/conference.phtml?option_lang=rus&eventID=31&confid=1794" class="text-secondary" target="_blank">All-Russian Optimization Seminar</a> about the recent paper <a href="https://arxiv.org/pdf/2110.04261.pdf" class="text-secondary" target="_blank">"Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity"</a> - joint work with <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.<br>[<a href="https://youtu.be/jRtiRXBOJ68" class="text-success" target="_blank">video (in Russian)</a>] [<a href="http://www.mathnet.ru:8080/PresentFiles/32776/last_iter_eg.pdf" class="text-secondary" target="_blank">slides</a>] [<a href="https://arxiv.org/abs/2110.04261" class="text-secondary" target="_blank">arXiv</a>]<br>---------------------------------------------------------</span></li><li><strong>[16 November, 2021]<br>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/pdf/2111.08611.pdf" class="text-secondary" target="_blank">"Stochastic Extragradient: General Analysis and Improved Rates"</a> - joint work with <a href="https://scholar.google.ru/citations?user=P5d_140AAAAJ&hl=en" class="text-secondary" target="_blank">Hugo Berard</a>, <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>, <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a>.<br><strong>Abstract: </strong><em>The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving min-max optimization and variational inequalities problems (VIP) appearing in various machine learning tasks. However, several important questions regarding the convergence properties of SEG are still open, including the sampling of stochastic gradients, mini-batching, convergence guarantees for the monotone finite-sum variational inequalities with possibly non-monotone terms, and others. To address these questions, in this paper, we develop a novel theoretical framework that allows us to analyze several variants of SEG in a unified manner. Besides standard setups, like Same-Sample SEG under Lipschitzness and monotonicity or Independent-Samples SEG under uniformly bounded variance, our approach allows us to analyze variants of SEG that were never explicitly considered in the literature before. Notably, we analyze SEG with arbitrary sampling which includes importance sampling and various mini-batching strategies as special cases. Our rates for the new variants of SEG outperform the current state-of-the-art convergence guarantees and rely on less restrictive assumptions.</em><br>---------------------------------------------------------</span></li><li><strong>[3 November, 2021]<br></strong>I gave a talk at&nbsp;<a href="https://sites.google.com/view/one-world-seminar-series-flow/archive/2021?authuser=0#h.xzu1f2fzzyz6" class="text-secondary text-primary" target="_blank" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">Federated Learning One-World</a><span style="font-size: 1.5rem;"> Seminar about the recent paper <a href="https://arxiv.org/pdf/2106.11257.pdf" class="text-secondary" target="_blank">"Secure Distributed Training at Scale"</a></span><span style="font-size: 1.5rem;">&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=HdwzsCMAAAAJ&hl=en" class="text-secondary">Alexander Borzunov</a>, <a href="https://scholar.google.ru/citations?user=LRKQhcYAAAAJ&hl=en" class="text-secondary" target="_blank">Michael Diskin</a>&nbsp;</span>and <a href="https://scholar.google.ru/citations?hl=en&user=930PERsAAAAJ" class="text-secondary" target="_blank">Max Ryabinin</a>.</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-sQ5BIKLCLn" id="image1-31">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 59%;">
                <img src="assets/images/flow-btard-talk-842x371.png" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-sQ5Bq0n9iF" id="content12-30">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>[<a href="https://youtu.be/VjyZQhlZGPk" class="text-success text-primary" target="_blank">video</a>] [<a href="https://drive.google.com/file/d/1KA30hIvEtPcOIc6jWhaCpE-aN438j2YZ/view" class="text-secondary text-primary" target="_blank">slides</a>] [<a href="https://arxiv.org/abs/2106.11257" class="text-secondary text-primary" target="_blank">arXiv</a>]<br>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-sMWSgp9Xz5" id="content12-2z">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><span style="font-size: 1.5rem;"><strong>[20 October, 2021]</strong><br><strong>Paper accepted to NeurIPS 2021 workshop:</strong><br>A short version of <a href="https://arxiv.org/pdf/2110.03294.pdf" class="text-secondary" target="_blank">"EF21 with Bells &amp; Whistles: Practical Algorithmic Extensions of Modern Error Feedback"</a> - joint work with <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en" class="text-secondary" target="_blank">lyas Fatkhullin</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov" class="text-secondary" target="_blank">Igor Sokolov</a>, <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>, and <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a> - was accepted to the <a href="https://opt-ml.org/index.html" class="text-secondary" target="_blank">NeurIPS 2021 workshop OPT2021</a>.<br>---------------------------------------------------------</span></li><li><strong>[15 October, 2021]<br></strong><strong>NeurIPS 2021 - Outstanding Reviewer Award:<br></strong>I got <a href="https://drive.google.com/file/d/11IC5dXptjiclVm3af0m8SPu9ahJ_Qt2Q/view?usp=sharing" class="text-secondary" target="_blank">"outstanding reviewer award''</a> (top 8 % of reviewers) and a free registration to the conference.</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-sMWSeYOkay" id="image1-2y">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 59%;">
                <img src="assets/images/neurips2021-outstanding-reviewer-1309x705.png" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-sMWSdeAsXu" id="content12-2x">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-swWlqbTb2L" id="content12-2w">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><span style="font-size: 1.5rem;"><strong>[8 October, 2021]<br>New paper out: </strong><a href="https://arxiv.org/pdf/2110.04261.pdf" class="text-secondary" target="_blank">"Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity"</a>&nbsp;- joint work with <a href="https://nicolasloizou.github.io/" class="text-secondary" target="_blank">Nicolas Loizou</a> and <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>.<br><strong>Abstract:</strong>&nbsp;<em>Extragradient method (EG) Korpelevich [1976] is one of the most popular methods for solving saddle point and variational inequalities problems (VIP). Despite its long history and significant attention in the optimization community, there remain important open questions about convergence of EG. In this paper, we resolve one of such questions and derive the first last-iterate O(1/K) convergence rate for EG for monotone and Lipschitz VIP without any additional assumptions on the operator. The rate is given in terms of reducing the squared norm of the operator. Moreover, we establish several results on the (non-)cocoercivity of the update operators of EG, Optimistic Gradient Method, and Hamiltonian Gradient Method, when the original operator is monotone and Lipschitz.</em><br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[7 October, 2021]<br>New paper out: </strong><a href="https://arxiv.org/pdf/2110.03294.pdf" class="text-secondary" target="_blank">"EF21 with Bells &amp; Whistles: Practical Algorithmic Extensions of Modern Error Feedback"</a>&nbsp;- joint work with <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en" class="text-secondary" target="_blank">lyas Fatkhullin</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov" class="text-secondary" target="_blank">Igor Sokolov</a>, <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>, and <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.<br><strong>Abstract: </strong><em>First proposed by Seide (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is O(1/T^{2/3}), the rate of gradient descent in the same regime is O(1/T)). Recently, Richtárik et al. (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21, all supported by strong convergence theory: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression. Several of these techniques were never analyzed in conjunction with EF before, and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.</em><br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[28 September, 2021]</strong><br><strong>Paper accepted to NeurIPS 2021:
</strong><br><a href="https://arxiv.org/pdf/2103.03239.pdf" class="text-secondary" target="_blank">"Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</a> - joint work with <a href="https://scholar.google.ru/citations?hl=en&user=930PERsAAAAJ" class="text-secondary" target="_blank">Max Ryabinin</a>, Vsevolod Plokhotnyuk, <a href="http://www.cs.toronto.edu/~pekhimenko/" class="text-secondary" target="_blank">Gennady Pekhimenko</a>. 
<br></span><strong>Abstract: </strong><span style="font-size: 1.5rem;"><em>Training deep neural networks on large datasets can often be accelerated by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-Reduce. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real-world applications, such as federated learning and cloud-based distributed training, operate on unreliable devices with unstable network bandwidth. As a result, these applications are restricted to using parameter servers or gossip-based averaging protocols. In this work, we lift that restriction by proposing Moshpit All-Reduce -- an iterative averaging protocol that exponentially converges to the global average. We demonstrate the efficiency of our protocol for distributed optimization with strong theoretical guarantees. The experiments show 1.3x speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5x speedup when training ALBERT-large from scratch using preemptible compute nodes.</em><br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[16 September, 2021]
<br></strong></span><span style="font-size: 1.5rem;"><strong>New scholarship: </strong>I received <a href="https://vk.com/wall-166330483_3226" class="text-secondary" target="_blank">A. M. Raigorodskii Scholarship for Contribution to the Development of Numerical Optimization Methods</a>. The scholarship has different levels and I won the main one. Great thanks to A. M. Raigorodskii for the support!<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[24 July, 2021]<br>ICML 2021 - top 10% of reviewers: </strong>I am among <a href="https://icml.cc/Conferences/2021/Reviewers" class="text-secondary" target="_blank">the top 10% of reviewers</a>. Free registration to the conference was a good bonus :)<br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[21 June, 2021]<br>New paper out: <a href="https://arxiv.org/pdf/2106.11257.pdf" class="text-secondary" target="_blank">"Secure Distributed Training at Scale"</a>&nbsp;</strong>- joint work with <a href="https://github.com/borzunov" class="text-secondary" target="_blank">Alexander Borzunov</a>, <a href="https://github.com/yhn112" class="text-secondary" target="_blank">Michael Diskin</a> <br>and <a href="https://scholar.google.ru/citations?hl=en&user=930PERsAAAAJ" class="text-secondary" target="_blank">Max Ryabinin</a>.<br><strong>Abstract: </strong><em>Some of the hardest problems in deep learning can be solved with the combined effort of many independent parties, as is the case for volunteer computing and federated learning. These setups rely on high numbers of peers to provide computational resources or train on decentralized datasets. Unfortunately, participants in such systems are not always reliable. Any single participant can jeopardize the entire training run by sending incorrect updates, whether deliberately or by mistake. Training in presence of such peers requires specialized distributed training algorithms with Byzantine tolerance. These algorithms often sacrifice efficiency by introducing redundant communication or passing all updates through a trusted server. As a result, it can be infeasible to apply such algorithms to large-scale distributed deep learning, where models can have billions of parameters. In this work, we propose a novel protocol for secure (Byzantine-tolerant) decentralized training that emphasizes communication efficiency. We rigorously analyze this protocol: in particular, we provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. To demonstrate its practical effectiveness, we conduct large-scale experiments on image classification and language modeling in presence of Byzantine attackers.<br>---------------------------------------------------------</em></span></li><li><span style="font-size: 1.5rem;"><strong>[10 June, 2021]<br>New paper out: <a href="https://arxiv.org/pdf/2106.05958.pdf" class="text-secondary" target="_blank">"Near-Optimal High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise"</a> </strong>- joint work with <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a>, Innokentiy Shibaev, <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=en" class="text-secondary" target="_blank">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Gasnikov</a>.<strong> 
<br></strong></span><strong>Abstract: </strong><span style="font-size: 1.5rem;"><em>Thanks to their practical efficiency and random nature of the data, stochastic first-order methods are standard for training large-scale machine learning models. Random behavior may cause a particular run of an algorithm to result in a highly suboptimal objective value, whereas theoretical guarantees are usually proved for the expectation of the objective value. Thus, it is essential to theoretically guarantee that algorithms provide small objective residual with high probability. Existing methods for non-smooth stochastic convex optimization have complexity bounds with the dependence on the confidence level that is either negative-power or logarithmic but under an additional assumption of sub-Gaussian (light-tailed) noise distribution that may not hold in practice, e.g., in several NLP tasks. In our paper, we resolve this issue and derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-smooth convex stochastic optimization problems with non-sub-Gaussian (heavy-tailed) noise. To derive our results, we propose novel stepsize rules for two stochastic methods with gradient clipping. Moreover, our analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, we provide an extension for strongly convex problems. Finally, our results imply that the first (accelerated) method we consider also has optimal iteration and oracle complexity in all the regimes, and the second one is optimal in the non-smooth setting.<br>---------------------------------------------------------</em></span></li><li><span style="font-size: 1.5rem;"><strong>[8 June, 2021]</strong><br><strong>Start of the internship at </strong><a href="https://mila.quebec/" class="text-secondary" target="_blank" style="font-weight: bold;">Mila, Montreal</a><strong>!</strong><br>I started my internship at the group of <a href="https://gauthiergidel.github.io/" class="text-secondary" target="_blank">Gauthier Gidel</a>. It is my pleasure to work with Gauthier! I will be here till the end of September.<br>---------------------------------------------------------</span></li><li><span style="font-size: 1.5rem;"><strong>[8 May, 2021]</strong><br><strong>Paper accepted to ICML 2021:<br></strong>&nbsp;<a href="https://arxiv.org/pdf/2102.07845.pdf" class="text-secondary" target="_blank">"MARINA: Faster Non-Convex Distributed Learning with Compression"</a> - joint work with&nbsp;
</span><a href="https://burlachenkok.github.io/home" class="text-secondary" target="_blank">Konstantin Burlachenko</a>, <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>, <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.<br><strong>Abstract:</strong> <em>We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.</em><br>---------------------------------------------------------</li><li><strong>[19 March, 2021]<br></strong><strong>ICLR 2021:</strong> I was recognized as an <em>"outstanding reviewer" </em>and got a free registration to the conference.</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-swWkbDzFBD" id="image1-2u">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 59%;">
                <img src="assets/images/iclr-2021-outstanding-reviewer-1309x349.jpg" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-swWkjzDRpL" id="content12-2v">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-srIeFH4KKX" id="content12-2t">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><strong>[10 March, 2021]<br></strong>I gave a talk at&nbsp;<a href="https://sites.google.com/view/one-world-seminar-series-flow/home" class="text-secondary" target="_blank" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">Federated Learning One-World</a><span style="font-size: 1.5rem;"> Seminar about the recent paper <a href="https://arxiv.org/pdf/2102.07845.pdf" class="text-secondary" target="_blank">"MARINA: Faster Non-Convex Distributed Learning with Compression"</a></span><span style="font-size: 1.5rem;">&nbsp;- joint work with <a href="https://burlachenkok.github.io/home" class="text-secondary" target="_blank">Konstantin Burlachenko</a>, <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>, and <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.</span></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-srIeEfIO9u" id="image1-2s">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 59%;">
                <img src="assets/images/flow-marina-868x440.png" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-srIeD9qRTl" id="content12-2r">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>[<a href="https://youtu.be/bj0E94Siq74" class="text-success" target="_blank">video</a>] [<a href="assets/files/MARINA_flow_talk.pdf" class="text-secondary" target="_blank">slides</a>] [<a href="https://arxiv.org/abs/2102.07845" class="text-secondary" target="_blank">arXiv</a>]<br>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-sr4veA1Hvx" id="content12-2o">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><strong>[4 March 2021]
<br></strong><strong>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/pdf/2103.03239.pdf" class="text-secondary" target="_blank">"Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</a></span><strong> </strong><span style="font-size: 1.5rem;">- joint work with <a href="https://scholar.google.ru/citations?hl=en&user=930PERsAAAAJ" class="text-secondary" target="_blank">Max Ryabinin</a>, Vsevolod Plokhotnyuk, <a href="http://www.cs.toronto.edu/~pekhimenko/" class="text-secondary" target="_blank">Gennady Pekhimenko</a></span><span style="font-size: 1.5rem;">.</span><strong>&nbsp;<br></strong><strong>Abstract: </strong><span style="font-size: 1.5rem;"><em>Training deep neural networks on large datasets can often be accelerated by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-Reduce. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real-world applications, such as federated learning and cloud-based distributed training, operate on unreliable devices with unstable network bandwidth. As a result, these applications are restricted to using parameter servers or gossip-based averaging protocols. In this work, we lift that restriction by proposing Moshpit All-Reduce -- an iterative averaging protocol that exponentially converges to the global average. We demonstrate the efficiency of our protocol for distributed optimization with strong theoretical guarantees. The experiments show 1.3x speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5x speedup when training ALBERT-large from scratch using preemptible compute nodes.</em><br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[26 February 2021]<br>New scholarship: </strong>I received <a href="https://mipt.ru/education/departments/fpmi/news/stipendiya_za_vklad_v_razvitie_chislennykh_metodov_optimizatsii" class="text-secondary" target="_blank">A. M. Raigorodskii Scholarship for Contribution to the Development of Numerical Optimization Methods</a>. The scholarship has different levels and I won the main one. Great thanks to A. M. Raigorodskii for the support!<br>---------------------------------------------------------
</li><li><strong>[15 February 2021]
<br>New paper out: </strong><a href="https://arxiv.org/pdf/2102.07845.pdf" class="text-secondary" target="_blank">"MARINA: Faster Non-Convex Distributed Learning with Compression"</a><strong> </strong>- joint work with <br><a href="https://burlachenkok.github.io/home" class="text-secondary" target="_blank">Konstantin Burlachenko</a>, <a href="https://zhizeli.github.io/" class="text-secondary" target="_blank">Zhize Li</a>, <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.
<br><strong>Abstract: </strong><span style="font-size: 1.5rem;"><em>We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.</em></span><span style="font-size: 1.5rem;"><em><br></em></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[10 February 2021]
<br></strong><strong>ICML 2021 Expert Reviewer invitation:</strong><span style="font-size: 1.5rem;">&nbsp; I accepted an invitation to serve as an expert reviewer for ICML 2021.</span></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-sr4vmq9l7b" id="image1-2p">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 59%;">
                <img src="assets/images/expert-reviewer-1309x309.jpg" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-sr4vn4i8tY" id="content12-2q">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-seZ62CBp4K" id="content12-2k">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><strong>[23 January 2021]<br>Paper <a href="https://aistats.org/aistats2021/accepted.html" class="text-secondary" target="_blank">accepted to AISTATS 2021</a>:<br>&nbsp;</strong><a href="https://arxiv.org/abs/2011.02828" class="text-secondary" target="_blank">"Local SGD: Unified Theory and New Efficient Methods"</a> - joint work with <a href="https://www.ttic.edu/faculty/hanzely/" class="text-secondary text-primary" target="_blank">Filip Hanzely</a> and
&nbsp;<span style="font-size: 1.5rem;"><a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.&nbsp;<br></span><strong>Abstract: </strong><span style="font-size: 1.5rem;"><em>We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions.&nbsp;<br></em></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[19 January 2021]
<br></strong><span style="font-size: 1.5rem;">I gave a talk "Linearly Converging Error Compensated SGD" at <a href="https://events.yandex.ru/events/neurips-afterparty2020" class="text-secondary" target="_blank">NeurIPS New Year AfterParty</a> (Yandex).<br>[<a href="assets/files/neurips_after_party_2021.pdf" class="text-secondary" target="_blank">slides</a>] [<a href="https://youtu.be/I5T8XBArcZ4?t=6610" class="text-success" target="_blank">video</a>] [<a href="https://papers.nips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html" class="text-secondary" target="_blank">paper</a>]<br></span>---------------------------------------------------------</li><li><strong>[11 December 2020]<br>New paper out: <a href="https://arxiv.org/pdf/2012.06188.pdf" class="text-secondary" target="_blank">"Recent Theoretical Advances in Non-Convex Optimization"</a> </strong>- joint work with <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a>, <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=en" class="text-secondary" target="_blank">Pavel Dvurechensky</a>, <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Gasnikov</a>, <a href="https://scholar.google.ru/citations?user=Ag6eTosAAAAJ&hl=en" class="text-secondary" target="_blank">Sergey Guminov</a>, <a href="https://scholar.google.ru/citations?user=CAq74XAAAAAJ&hl=en" class="text-secondary" target="_blank">Dmitry Kamzolov</a> and Innokentiy Shibaev.<br><strong>Abstract:</strong> <em>Motivated by recent increased interest in optimization algorithms for non-convex optimization in application to training deep neural networks and other optimization problems in data analysis, we give an overview of recent theoretical results on global performance guarantees of optimization algorithms for non-convex optimization. We start with classical arguments showing that general non-convex problems could not be solved efficiently in a reasonable time. Then we give a list of problems that can be solved efficiently to find the global minimizer by exploiting the structure of the problem as much as it is possible. Another way to deal with non-convexity is to relax the goal from finding the global minimum to finding a stationary point or a local minimum. For this setting, we first present known results for the convergence rates of deterministic first-order methods, which are then followed by a general theoretical analysis of optimal stochastic and randomized gradient schemes, and an overview of the stochastic first-order methods. After that, we discuss quite general classes of non-convex problems, such as minimization of α-weakly-quasi-convex functions and functions that satisfy Polyak-Lojasiewicz condition, which still allow obtaining theoretical convergence guarantees of first-order methods. Then we consider higher-order and zeroth-order/derivative-free methods and their convergence rates for non-convex optimization problems.<br>---------------------------------------------------------</em></li><li><strong>[6-12 December 2020]</strong><br><strong>I attended&nbsp;</strong><a href="https://neurips.cc/virtual/2020/public/index.html" class="text-secondary" target="_blank" style="font-weight: bold;">NeurIPS 2020</a><strong>!</strong><br>The conference was organized perfectly. In particular,&nbsp;poster sessions in <a href="https://gather.town/" class="text-secondary" target="_blank">Gather Town</a>&nbsp;were great: this tool made them as vivid as possible. I presented the paper <a href="https://neurips.cc/virtual/2020/public/poster_ef9280fbc5317f17d480e4d4f61b3751.html" class="text-secondary" target="_blank">"Linearly Converging Error Compensated SGD"</a> - joint work with <a href="https://www.dmitry-kovalev.com/" class="text-secondary" target="_blank">Dmitry Kovalev</a>, Dmitry Makarenko and <a href="https://richtarik.org/" class="text-secondary" target="_blank">
Peter Richtárik</a>. <a href="https://marinadanya.github.io/" class="text-secondary" target="_blank">Marina Danilova</a> presented our joint work with <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Gasnikov</a> - <a href="https://neurips.cc/virtual/2020/public/poster_abd1c782880cc59759f4112fda0b8f98.html" class="text-secondary" target="_blank">"Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping"</a>.<br>[<a href="assets/files/clipped_sstm.pdf" class="text-secondary" target="_blank">poster</a>] [<a href="assets/files/ef_sigma_k_poster.pdf" class="text-secondary" target="_blank">poster</a>]<br>---------------------------------------------------------</li><li><strong>[26 November 2020]<br>New paper out: <a href="https://arxiv.org/pdf/2011.13259.pdf" class="text-secondary" target="_blank">"Recent theoretical advances in decentralized distributed convex optimization"</a>&nbsp; </strong>- joint work with <a href="https://scholar.google.ru/citations?user=sEjyzkgAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Rogozin</a>, <a href="https://anbeznosikov.github.io/" class="text-secondary" target="_blank">Aleksandr Beznosikov</a>, <a href="https://scholar.google.ru/citations?user=5ILnTRsAAAAJ&hl=en" class="text-secondary" target="_blank">Darina Dvinskikh</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" class="text-secondary" target="_blank">Alexander Gasnikov</a>.<br><strong>Abstract: </strong><em>In the last few years, the theory of decentralized distributed convex optimization has made significant progress. The lower bounds on communications rounds and oracle calls have appeared, as well as methods that reach both of these bounds. In this paper, we focus on how these results can be explained based on optimal algorithms for the non-distributed setup. In particular, we provide our recent results that have not been published yet, and that could be found in details only in arXiv preprints.<br>---------------------------------------------------------</em></li><li><strong>[3 November, 2020]<br>New paper out: <a href="https://arxiv.org/pdf/2011.02828.pdf" class="text-secondary" target="_blank">"Local SGD: Unified Theory and New Efficient Methods"</a>&nbsp;</strong>- joint work with <a href="https://fhanzely.github.io/index.html" class="text-secondary" target="_blank">Filip Hanzely</a> and<br><a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.<br><strong>Abstract: </strong><em>We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions.<br>---------------------------------------------------------</em></li><li><strong>[23 October, 2020]</strong><br><strong>New paper out (Spotlight @ <a href="https://neurips.cc/Conferences/2020/AcceptedPapersInitial" class="text-secondary" target="_blank">NeurIPS 2020</a>):</strong><br><a href="https://arxiv.org/pdf/2010.12292.pdf" class="text-secondary" target="_blank">"Linearly Converging Error Compensated SGD"</a> - joint work with <a href="https://www.dmitry-kovalev.com/" class="text-secondary" target="_blank">Dmitry Kovalev</a>, Dmitry Makarenko, and <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.<br><strong>Abstract: </strong><em>In this paper, we propose a unified analysis of variants of distributed SGD with arbitrary compressions and delayed updates. Our framework is general enough to cover different variants of quantized SGD, Error-Compensated SGD (EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive the complexity results for all the methods that fit our framework. For the existing methods, this theorem gives the best-known complexity results. Moreover, using our general scheme, we develop new variants of SGD that combine variance reduction or arbitrary sampling with error feedback and quantization and derive the convergence rates for these methods beating the state-of-the-art results. In order to illustrate the strength of our framework, we develop 16 new methods that fit this. In particular, we propose the first method called EC-SGD-DIANA that is based on error-feedback for biased compression operator and quantization of gradient differences and prove the convergence guarantees showing that EC-SGD-DIANA converges to the exact optimum asymptotically in expectation with constant learning rate for both convex and strongly convex objectives when workers compute full gradients of their loss functions. Moreover, for the case when the loss function of the worker has the form of finite sum, we modified the method and got a new one called EC-LSVRG-DIANA which is the first distributed stochastic method with error feedback and variance reduction that converges to the exact optimum asymptotically in expectation with a constant learning rate.</em><br>---------------------------------------------------------</li><li><strong>[21 October, 2020]</strong><br><strong>I am among the top 10% reviewers at NeurIPS 2020!</strong><br>I am happy to contribute to the community through reviewing and extremely happy and proud that my work as a reviewer was highly evaluated. Free registration is a nice bonus :)<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-seZ6bQgqyY" id="image1-2l">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 59%;">
                <img src="assets/images/top-10-percent-1-799x259.png" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-seZ6huhZ6b" id="content12-2m">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-seZ0QX32QA" id="content12-2f">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><strong>[7 October, 2020]<br></strong>I gave a talk on <a href="https://sites.google.com/view/one-world-seminar-series-flow/home" class="text-secondary" target="_blank" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">Federated Learning One-World</a><span style="font-size: 1.5rem;"> Seminar about the recent paper </span><a href="https://arxiv.org/pdf/2010.12292.pdf" class="text-secondary" target="_blank" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">"Linearly Converging Error Compensated SGD"</a><span style="font-size: 1.5rem;">&nbsp;- joint work with <a href="https://www.dmitry-kovalev.com/" class="text-secondary" target="_blank">Dmitry Kovalev</a>, Dmitry Makarenko, and <a href="https://richtarik.org/index.html" class="text-secondary" target="_blank">Peter Richtárik</a>.</span></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-seZ1T2aAaB" id="image1-2g">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 57%;">
                <img src="assets/images/flow-talk-1-799x497.png" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-seZ2zs18yx" id="content12-2j">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li>[<a href="https://www.youtube.com/watch?v=3oSq1FWDs-0&feature=emb_logo" class="text-success" target="_blank">video</a>] [<a href="https://drive.google.com/file/d/1NSomfOF2emkb2djXxGINBnI5vkM3tzcA/view" class="text-secondary" target="_blank">slides</a>] [<a href="https://arxiv.org/abs/2010.12292" class="text-secondary" target="_blank">arXiv</a>]<br>---------------------------------------------------------<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-qYQrUl0BTQ" id="content12-15">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li></ul><ul><li><strong>[26 September, 2020]</strong><br><strong>Two papers <a href="https://neurips.cc/Conferences/2020/AcceptedPapersInitial" class="text-secondary" target="_blank">accepted</a> to </strong><a href="https://neurips.cc/" target="_blank" class="text-secondary" style="font-weight: bold;">NeurIPS 2020</a><strong>!</strong><br><br>1. <strong>"Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping" </strong>- joint work with <br><a href="https://marinadanya.github.io/" target="_blank" class="text-secondary">Marina Danilova</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a> - got accepted for a poster presentation.<br>[<a href="https://arxiv.org/abs/2005.10785" target="_blank" class="text-secondary">arXiv</a>] [<a href="https://drive.google.com/file/d/1D0FDyhtHHdSZA4ngwznRPXwUXhA15vLC/view" target="_blank" class="text-success">video from MLSS 2020</a>] [<a href="assets/files/MLSS_2020_poster.pdf" target="_blank" class="text-secondary">slides from MLSS 2020</a>]<br><br>2. <strong>"Linearly Converging Error Compensated SGD" </strong>- joint work with <a href="https://www.dmitry-kovalev.com/" target="_blank" class="text-secondary">Dmitry Kovalev</a>, Dmitry Makarenko and <br><a href="https://richtarik.org/" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;- got accepted for a spotlight presentation. The paper will be available on arXiv soon.<br>[<a href="https://drive.google.com/file/d/1Mz9AyGLXGvfxPf__wX5xMRUe4HGyTY5E/view" target="_blank" class="text-success">video from MLSS 2020</a>] [<a href="https://drive.google.com/file/d/19WMHzPm6yBQt5rKpuAF1Avf802dzUVA8/view" target="_blank" class="text-secondary">slides from MLSS 2020</a>]<br><br>I would like to thank my great co-authors!<br>---------------------------------------------------------</li><li><strong>[22 September, 2020]</strong><br>It is my pleasure to join<strong> Yandex.Research-MIPT Lab </strong>as a junior researcher!<br>---------------------------------------------------------</li><li><strong>[17 September, 2020]<br>Teaching in the Fall 2020 semester.</strong> This semester I will be one of the lecturers for a fresh course "Optimization methods at ML" at MIPT that we developed together with <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>, <a href="https://marinadanya.github.io/" target="_blank" class="text-secondary">Marina Danilova</a>, Alexander Maslovsky, and <a href="https://scholar.google.ru/citations?hl=en&user=sEjyzkgAAAAJ" target="_blank" class="text-secondary">Alexander Rogozin</a>.<br>---------------------------------------------------------</li><li><strong>[1 September, 2020]</strong><br>My remote internship at <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>'s group has started today and will last for 6 months! It is a great pleasure for me to be a part of Peter's group again!<br>---------------------------------------------------------</li><li><strong>[26-28 August, 2020]
<br></strong><span style="font-size: 1.5rem;">I am attending <a href="https://www.aistats.org/" target="_blank" class="text-secondary">AISTATS 2020</a>, which is held virtually this year for the reasons you all know.</span><strong>&nbsp;</strong><span style="font-size: 1.5rem;">I am glad to present our joint work with <a href="https://fhanzely.github.io/index.html" target="_blank" class="text-secondary">Filip Hanzely</a>, and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;called <a href="http://proceedings.mlr.press/v108/gorbunov20a.html" target="_blank" class="text-secondary">"A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent"</a>.<br></span><span style="font-size: 1.5rem;">[<a href="https://aistats2020.net/poster_270.html" target="_blank" class="text-success">video</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[13 August, 2020]
</strong><br><strong>Great news:</strong> our paper <a href="https://arxiv.org/pdf/1804.02394.pdf" target="_blank" class="text-secondary">"An Accelerated Directional Derivative Method for Smooth Stochastic Convex Optimization"</a> with <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=en" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a> <strong>was accepted to <a href="https://www.sciencedirect.com/science/article/pii/S0377221720307402?casa_token=a02-zmFXPE8AAAAA:V1xDk2RIbTD8V0V9dC9QiyyD-CiU-fui41i6yTVHYsaFDjp08oaGsyo2-5N7Zw9ES6o9Pn0DKOr-" target="_blank" class="text-secondary">European Journal of Operational Research!</a></strong><a href="https://www.sciencedirect.com/science/article/pii/S0377221720307402?casa_token=a02-zmFXPE8AAAAA:V1xDk2RIbTD8V0V9dC9QiyyD-CiU-fui41i6yTVHYsaFDjp08oaGsyo2-5N7Zw9ES6o9Pn0DKOr-" target="_blank"> </a><br>---------------------------------------------------------</li><li><strong>[3 August, 2020]</strong><br>I am happy to announce that our paper <a href="https://arxiv.org/abs/1902.03591" target="_blank" class="text-secondary">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a> with <a href="https://ehbergou.github.io/" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;<strong>was accepted to SIAM Journal on Optimization!</strong><br>---------------------------------------------------------</li><li><strong>[2 August, 2020]</strong><br>I have arrived at <a href="https://sochisirius.ru/uploads/f/SiriusAnnualReport2019_en.pdf" target="_blank" class="text-secondary">Sirius university</a> (Sochi, Russia). I will give here 2 lectures and will be a mentor for a couple of students research projects on optimization. I will be here till the end of August.<br>---------------------------------------------------------</li><li><strong>[8 July, 2020] 
<br></strong><span style="font-size: 1.5rem;">I gave a <a href="http://www.mathnet.ru/php/seminars.phtml?option_lang=rus&presentid=27382" target="_blank" class="text-secondary">talk</a> "On the convergence of SGD-like methods for convex and non-convex optimization problems"&nbsp;on <a href="http://www.mathnet.ru/php/conference.phtml?option_lang=rus&eventID=31&confid=1794" class="text-secondary">Russian Optimization Seminar</a>.<br></span><span style="font-size: 1.5rem;">[<a href="https://www.youtube.com/watch?time_continue=7689&v=Jz3K_dKPJUA&feature=emb_logo" target="_blank" class="text-success">video</a>] [<a href="assets/files/all_russian_seminar_8_july.pdf" target="_blank" class="text-secondary">slides</a>]&nbsp;<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[28 June - 10 July, 2020]
<br></strong><span style="font-size: 1.5rem;">It was my pleasure to take part in&nbsp;<strong>online</strong>&nbsp;<a href="http://mlss.tuebingen.mpg.de/2020/" target="_blank" class="text-secondary">Machine Learning Summer School&nbsp;2020</a>. I have met a lot of interesting people, listened to amazing lectures, and taken part in several round tables. Finally, I have presented our joint work with <a href="https://www.dmitry-kovalev.com/" target="_blank" class="text-secondary">Dmitry Kovalev</a>, Dmitry Makarenko and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a> called "Linearly Converging Error Compensated SGD".<br>[<a href="https://drive.google.com/file/d/1Mz9AyGLXGvfxPf__wX5xMRUe4HGyTY5E/view?usp=sharing" target="_blank" class="text-success">video</a>] [<a href="https://drive.google.com/file/d/19WMHzPm6yBQt5rKpuAF1Avf802dzUVA8/view?usp=sharing" target="_blank" class="text-secondary">slides</a>]<br><br><a href="https://marinadanya.github.io/" target="_blank" class="text-secondary">Marina Danilova</a> also presented our joint work with <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>&nbsp;called <a href="https://arxiv.org/pdf/2005.10785.pdf" target="_blank" class="text-secondary">"Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping"</a>.<br>[<a href="https://drive.google.com/file/d/1D0FDyhtHHdSZA4ngwznRPXwUXhA15vLC/view" target="_blank" class="text-success">video</a>] [<a href="assets/files/MLSS_2020_poster.pdf" target="_blank" class="text-secondary">slides</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[16 June, 2020]
<br></strong><span style="font-size: 1.5rem;"><strong>I have defended my Master thesis at MIPT!</strong><br><span style="font-size: 1.5rem;">---------------------------------------------------------</span><br></span></li><li><strong>[22 May, 2020]</strong><br><strong>New paper out: <a href="https://arxiv.org/pdf/2005.10785.pdf" target="_blank" class="text-secondary">"</a></strong><a href="https://arxiv.org/pdf/2005.10785.pdf" target="_blank" class="text-secondary">Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping"</a>&nbsp;- joint work with <a href="https://marinadanya.github.io/" target="_blank" class="text-secondary">Marina Danilova</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br><strong>Abstract: </strong><em>In this paper, we propose a new accelerated stochastic first-order method called clipped-SSTM for smooth convex stochastic optimization with heavy-tailed distributed noise in stochastic gradients and derive the first high-probability complexity bounds for this method closing the gap in the theory of stochastic optimization with heavy-tailed noise. Our method is based on a special variant of accelerated Stochastic Gradient Descent (SGD) and clipping of stochastic gradients. We extend our method to the strongly convex case and prove new complexity bounds that outperform state-of-the-art results in this case. Finally, we extend our proof technique and derive the first non-trivial high-probability complexity bounds for SGD with clipping without light-tails assumption on the noise.</em><br>---------------------------------------------------------</li><li><strong>[29 April, 2020]</strong><br>I got accepted to <a href="http://mlss.tuebingen.mpg.de/2020/" class="text-secondary" target="_blank">The Machine Learning Summer School</a> this year which will be fully virtual. There were more than 1300 applications and only 180 available slots, so, I am incredibly pleased to be one of those 180 students and looking forward to taking part in this school.<br>---------------------------------------------------------</li><li><strong>[27-30 April, 2020]</strong><br>I took part in the first fully virtual conference - <a href="https://iclr.cc/virtual_2020/index.html" target="_blank" class="text-secondary">ICLR 2020</a>!<br>I have presented our paper <a href="http://www.openreview.net/pdf?id=HylAoJSKvH" target="_blank" class="text-secondary">"A Stochastic Derivative Free Optimization Method with Momentum"</a> - joint work with <a href="https://www.adelbibi.com/" target="_blank" class="text-secondary">Adel Bibi</a>, <a href="http://ozansener.net/" class="text-secondary">Ozan Sener</a>, <a href="https://ehbergou.github.io/" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>I enjoyed the online format and met a lot of amazing people there. Great thanks to the organizers!<br>[<a href="https://iclr.cc/virtual_2020/poster_HylAoJSKvH.html" target="_blank" class="text-success">video</a>]<br>---------------------------------------------------------</li><li><strong>[2 February, 2020]<br></strong>I have arrived at KAUST. I will be here till the end of March to work with <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;as a visiting student.<br><strong><br>Update: </strong>due to the situation with COVID-19 I have decided to come back to Moscow earlier (no, there is no confirmed cases at KAUST). I have arrived in Moscow on 14 March.<br>---------------------------------------------------------</li><li><strong>[15 January, 2020]</strong><br><strong>I got <a href="https://vk.com/wall-44001716_4739" target="_blank" class="text-secondary">Huawei scholarship</a>&nbsp;for MIPT bachelor and master students for academic and reseacrh achievements!</strong><br>The prize is 125 000 Russian rubles.<br>---------------------------------------------------------</li><li><strong>[07 January, 2020]</strong><br><strong>Paper accepted to AISTATS 2020!</strong><br>Our paper <a href="https://arxiv.org/abs/1905.11261" target="_blank" class="text-secondary">"A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent"</a>&nbsp;- joint work with <a href="https://fhanzely.github.io/index.html" target="_blank" class="text-secondary">Filip Hanzely</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;- got accepted to <a href="https://www.aistats.org/" target="_blank" class="text-secondary">AISTATS 2020</a>. The conference will take place in Palermo, Sicily, Italy during June 3-5.<br>---------------------------------------------------------</li><li><strong>[20 December, 2019]</strong><br><strong>Paper accepted to ICLR 2020!</strong><br>Our paper <a href="https://arxiv.org/abs/1905.13278" target="_blank" class="text-secondary">"A Stochastic Derivative Free Optimization Method with Momentum"</a> - joint work with <a href="http://www.adelbibi.com/" target="_blank" class="text-secondary">Adel Bibi</a>, <a href="http://ozansener.net/" target="_blank" class="text-secondary">Ozan Sener</a>, <a href="https://ehbergou.github.io/" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a> - got accepted to <a href="https://iclr.cc/Conferences/2020/" target="_blank" class="text-secondary">ICLR 2020</a>. The conference will take place in Addis Ababa, Ethiopia during April 26-30.<br><br>The paper was presented last week at the <a href="https://optrl2019.github.io/" target="_blank" class="text-secondary">NeurIPS 2019 Optimization Foundations of Reinforcement Learning Workshop</a>, here is <a href="assets/files/SMTP_OptRL_NeurIPS2019_poster.pdf" target="_blank" class="text-secondary">our poster</a>.<br>---------------------------------------------------------</li><li><strong>[26 November, 2019]<br>New paper out: <a href="https://arxiv.org/pdf/1911.10645.pdf" target="_blank" class="text-secondary">"Derivative-Free Method For Decentralized Distributed Non-Smooth Optimization"</a>&nbsp;</strong>-<strong>&nbsp;</strong>joint work with Aleksandr Beznosikov and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br><strong>Abstract:</strong>&nbsp;<em>In this paper, we propose new derivative-free method which is based on the Sliding Algorithm from Lan (2016, 2019) for the convex composite optimization problem that includes two terms: smooth one and non-smooth one. We prove the convergence rate for the new method that matches the corresponding rate for the first-order method up to a factor proportional to the dimension of the space. We apply this method for the decentralized distributed optimization and prove the bounds for the number of communication rounds for this method that matches the lower bounds. We prove the bound for the number of zeroth-order oracle calls per node that matches the similar state-of-the-art bound for the first-order decentralized distributed optimization up to to the factor proportional to the dimension of the space.<br>---------------------------------------------------------</em></li><li><strong>[20 November, 2019]<br>New paper out: </strong><a href="https://arxiv.org/pdf/1911.07363.pdf" target="_blank" class="text-secondary">"Optimal Decentralized Distributed Algorithms for Stochastic Convex Optimization"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=5ILnTRsAAAAJ&hl=en" target="_blank" class="text-secondary">Darina Dvinskikh</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br><strong>Abstract: </strong><em>We consider stochastic convex optimization problems with affine constraints and develop several methods using either primal or dual approach to solve it. In the primal case, we use special penalization technique to make the initial problem more convenient for using optimization methods. We propose algorithms to solve it based on Similar Triangles Method with Inexact Proximal Step for the convex smooth and strongly convex smooth objective functions and methods based on Gradient Sliding algorithm to solve the same problems in the non-smooth case. We prove the convergence guarantees in the smooth convex case with deterministic first-order oracle.&nbsp;
<br><span style="font-size: 1.5rem;"><br>We propose and analyze three novel methods to handle stochastic convex optimization problems with affine constraints: SPDSTM, R-RRMA-AC-SA2 and SSTM_sc. All methods use stochastic dual oracle. SPDSTM is the stochastic primal-dual modification of STM and it is applied for the dual problem when the primal functional is strongly convex and Lipschitz continuous on some ball. We extend the result from Dvinskikh &amp; Gasnikov (2019) for this method to the case when only biased stochastic oracle is available. R-RRMA-AC-SA2 is an accelerated stochastic method based on the restarts of RRMA-AC-SA2 from Foster et al. (2019) and SSTM_sc is just stochastic STM for strongly convex problems. Both methods are applied to the dual problem when the primal functional is strongly convex, smooth and Lipschitz continuous on some ball and use stochastic dual first-order oracle. We develop convergence analysis for these methods for the unbiased and biased oracles respectively. &nbsp;<br></span></em><span style="font-size: 1.5rem;"><em><br>Finally, we apply all aforementioned results and approaches to solve the decentralized distributed optimization problem and discuss the optimality of the obtained results in terms of communication rounds and the number of oracle calls per node.</em><br>---------------------------------------------------------</span></li><li><strong>[18 October, 2019]<br></strong>Today I gave a talk based on <a href="https://arxiv.org/abs/1905.11261" target="_blank" class="text-secondary">our recent paper</a> with <a href="https://fhanzely.github.io/index.html" target="_blank" class="text-secondary">Filip Hanzely</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;at <a href="https://www.di.ens.fr/sierra/index.php" target="_blank" class="text-secondary">SIERRA, INRIA</a>. <br><a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter</a>, thanks for <a href="https://richtarik.org/talks/ZOO_course_ICCOPT2019.pdf" target="_blank" class="text-secondary">your slides</a>&nbsp;that you prepared for <a href="https://iccopt2019.berlin/index.php?page=summerschool" target="_blank" class="text-secondary">ICCOPT 2019 Summer School</a>.<br>[<a href="assets/files/unified_sgd_inria2019.pdf" target="_blank" class="text-secondary">slides</a>]<br>---------------------------------------------------------</li><li><strong>[06 October - 26 October, 2019]</strong><br>I am visiting <a href="https://www.di.ens.fr/~ataylor/" target="_blank" class="text-secondary">Adrien Taylor</a> and <a href="https://www.di.ens.fr/~fbach/" target="_blank" class="text-secondary">Francis Bach</a> at <a href="https://www.di.ens.fr/sierra/index.php" target="_blank" class="text-secondary">SIERRA, INRIA</a>.&nbsp;<br>---------------------------------------------------------</li><li><strong>[02 October, 2019]
<br></strong><strong>2 papers got accepted to NeurIPS workshops: <br></strong><span style="font-size: 1.5rem;"><br>1)&nbsp;<em><a href="https://arxiv.org/abs/1802.09022" target="_blank" class="text-secondary">"An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization"</a>&nbsp;</em>- joint work with <a href="https://www.wias-berlin.de/people/dvureche/?lang=1" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a> - got accepted to the workshop <a href="https://sites.google.com/site/optneurips19/" target="_blank" class="text-secondary">"Beyond First Order Methods in ML"</a><br><br>2) <a href="https://arxiv.org/abs/1905.13278" target="_blank" class="text-secondary">"A Stochastic Derivative Free Optimization Method with Momentum"</a> - joint work with <a href="http://www.adelbibi.com/" target="_blank" class="text-secondary">Adel Bibi</a>, <a href="http://ozansener.net/" target="_blank" class="text-secondary">Ozan Sener</a>, <a href="https://ehbergou.github.io/" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a></span>&nbsp;- got accepted to the workshop <a href="https://optrl2019.github.io/accepted_papers.html" target="_blank" class="text-secondary">"Optimization Foundations for Reinforcement Learning"</a> <br>---------------------------------------------------------&nbsp;</li><li><strong>[23 June, 2019]<br>New book out </strong>(in Russian):<strong> <a href="https://arxiv.org/pdf/1907.01060.pdf" target="_blank" class="text-secondary">"Lecture Notes on Stochastic Processes" </a></strong>- joint work with <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=en" target="_blank" class="text-secondary">Alexander Gasnikov</a>, <a href="https://scholar.google.ru/citations?hl=en&user=_O_mjxEAAAAJ" target="_blank" class="text-secondary">Sergey Guz</a>, Elena Chernousova, <a href="https://scholar.google.ru/citations?hl=en&user=FavY01MAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" class="text-secondary">Maksim Shirobokov</a>, <a href="https://scholar.google.ru/citations?user=XlmSx18AAAAJ&hl=en" target="_blank" class="text-secondary">Egor Shulgin</a>.<br>---------------------------------------------------------</li><li><strong>[30 May, 2019] 
<br></strong><strong>New paper out: <a href="https://arxiv.org/pdf/1905.13278.pdf" target="_blank" class="text-secondary">"A Stochastic Derivative Free Optimization Method with Momentum"</a> - </strong><span style="font-size: 1.5rem;">joint work with <a href="http://www.adelbibi.com" target="_blank" class="text-secondary">Adel Bibi</a>, <a href="http://ozansener.net" target="_blank" class="text-secondary">Ozan Sener</a>, <a href="https://ehbergou.github.io" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>.&nbsp;<br></span><span style="font-size: 1.5rem;"><strong>Abstract: </strong><em>We consider the problem of unconstrained minimization of a smooth objective function in R^d in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP). We show new complexity results for non-convex, convex and strongly convex functions.  We test our method on a collection of learning to continuous control tasks on several MuJoCo environments with varying difficulty andcompare against STP, other state-of-the-art derivative-free optimization algorithms and against policy gradient methods. SMTP significantly outperforms STP and all other methods that we considered in our numerical experiments. Our second contribution is SMTP with importance sampling which we call SMTP_IS. We provide convergence analysis of this method for non-convex, convex and strongly convex objectives.</em></span><strong>&nbsp;<br></strong>---------------------------------------------------------</li><li><strong>[27 May, 2019]
<br></strong><strong>New paper out: <a href="https://arxiv.org/pdf/1905.11261.pdf" target="_blank" class="text-secondary">"A Unified Theory of SGD: Variance Reduction,&nbsp;</a></strong><strong><a href="https://arxiv.org/pdf/1905.11261.pdf" target="_blank" class="text-secondary">Sampling, Quantization and Coordinate Descent"</a>&nbsp;</strong><span style="font-size: 1.5rem;">- joint work with <a href="https://fhanzely.github.io/index.html" target="_blank" class="text-secondary">Filip Hanzely</a> and&nbsp;</span><span style="font-size: 1.5rem;"><a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>.<br><strong>Abstract: </strong><em>In this paper we introduce a unified analysis of a large family of variants of proximal stochastic gradient descent (SGD) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling.  As a by-product, we obtain the first unified theory of SGD and randomized coordinate descent (RCD) methods,  the first unified theory of variance reduced and non-variance-reduced SGD methods, and the first unified theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop five new variants of SGD, and through numerical experiments demonstrate some of their properties.</em><br>---------------------------------------------------------<br></span></li><li><strong>[10 April, 2019]<br>I got the Ilya Segalovich Award - Yandex scientific scholarship! </strong>The award includes scholarship (total amount - 350,000 Russian rubles), insternship offer at Yandex.Research, travel grant to attend one international conference and personal mentor from Yandex.<br>[<a href="https://www.youtube.com/embed/6qkme8n3UI8" target="_blank" class="text-success">video</a>] [<a href="https://yandex.com/scholarships/scholars" target="_blank">about award</a>] [<a href="https://habr.com/ru/company/yandex/blog/447708/" target="_blank" class="text-secondary">about winners</a>]<br>---------------------------------------------------------</li><li><strong>[23 March, 2019]<br>New paper out: <a href="https://arxiv.org/pdf/1903.09844.pdf" target="_blank" class="text-secondary">"On Dual Approach for Distributed Stochastic Convex Optimization over Networks"</a>&nbsp;- </strong>joint work with <a href="https://scholar.google.ru/citations?user=5ILnTRsAAAAJ&hl=ru" target="_blank" class="text-secondary">Darina Dvinskikh</a>, <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>, <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=ru" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://cauribe.mit.edu" target="_blank" class="text-secondary">Cesar A. Uribe</a>.<br>Abstract: <em>We introduce dual stochastic gradient oracle methods for distributed stochastic convex optimization problems over networks. We estimate the complexity of the proposed method in terms of probability of large deviations. This analysis is based on a new technique that allows to bound the distance between the iteration sequence and the solution point. By the proper choice of batch size, we can guarantee that this distance equals (up to a constant) to the distance between the starting point and the solution.<br>---------------------------------------------------------</em></li><li><strong>[10 February, 2019]
<br></strong><strong>New paper out:  </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/pdf/1902.03591.pdf" target="_blank" class="text-secondary">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a> - joint work with &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<a href="http://maiage.jouy.inra.fr/?q=fr/bergou" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.&nbsp;<br></span><span style="font-size: 1.5rem;">Abstract: <em>In this paper we consider the unconstrained minimization problem of a smooth function in R^n in a setting where only function evaluations are possible. We design a novel randomized derivative-free algorithm --- the stochastic three points (STP) method --- and analyze its iteration complexity. At each iteration, STP generates a random search direction according to a certain fixed probability law. Our assumptions on this law are very mild: roughly speaking, all laws which do not concentrate all measure on any halfspace passing through the origin will work. For instance, we allow for the uniform distribution on the sphere and also distributions that concentrate all measure on a positive spanning set.
<br></em></span><em>Given a current iterate x, STP compares the objective function at three points: x, x+αs and x−αs, where α&gt;0 is a stepsize parameter and s is the random search direction. The best of these three points is the next iterate. We analyze the method STP under several stepsize selection schemes (fixed, decreasing, estimated through finite differences, etc). We study non-convex, convex and strongly convex cases.&nbsp;<br></em><em>We also propose a parallel version for STP, with iteration complexity bounds which do not depend on the dimension n.&nbsp;<br></em>arXiv: <a href="https://arxiv.org/abs/1902.03591" target="_blank" class="text-secondary" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">1901.09269</a><span style="font-size: 1.5rem;">&nbsp;<br></span>---------------------------------------------------------</li><li><strong>[08 February, 2019]</strong><br>I am a reviewer for <a href="https://www.icml.cc" target="_blank" class="text-secondary">ICML 2019</a>! This is my first time being a reviewer for such a conference.<br>---------------------------------------------------------</li><li><strong>[26 January, 2019]<br>New paper out: &nbsp;</strong><a href="https://arxiv.org/pdf/1901.09269.pdf" target="_blank" class="text-secondary">"Distributed learning with compressed gradient differences"</a> - joint work with <a href="https://konstmish.github.io" target="_blank" class="text-secondary">Konstantin Mishchenko</a>, <a href="http://mtakac.com" target="_blank" class="text-secondary">Martin Takáč</a> and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>Abstract: <em>Training very large machine learning models requires a distributed computing approach, with communication of the model updates often being the bottleneck. For this reason, several methods based on the compression (e.g., sparsification and/or quantization) of the updates were recently proposed, including QSGD (Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods are able to learn the gradients, which means that they necessarily suffer from several issues, such as the inability to converge to the true optimum in the batch mode, inability to work with a nonsmooth regularizer, and slow convergence rates. In this work we propose a new distributed learning method - DIANA - which resolves these issues via compression of gradient differences. We perform a theoretical analysis in the strongly convex and nonconvex settings and show that our rates are vastly superior to existing rates. Our analysis of block quantization and differences between l2 and l∞ quantization closes the gaps in theory and practice. Finally, by applying our analysis technique to TernGrad, we establish the first convergence rate for this method.</em><br>arXiv: <a href="https://arxiv.org/abs/1901.09269" target="_blank" class="text-secondary">1901.09269</a><br>---------------------------------------------------------</li><li><strong>[13 January - 24 February, 2019]
<br></strong><span style="font-size: 1.5rem;">I am visiting <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a> at <a href="https://vcc.kaust.edu.sa/Pages/Home.aspx" target="_blank" class="text-secondary">KAUST</a>.&nbsp;<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------&nbsp;</span></li><li><strong>[05 September, 2018]</strong><br>The paper "<a href="https://arxiv.org/abs/1802.03703" target="_blank" class="text-secondary">Stochastic spectral and conjugate descent methods</a>" - joint work with Dmitry Kovalev, Elnur Gasanov and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a> &nbsp;- accepted to <a href="https://nips.cc" target="_blank" class="text-secondary">The Thirty-second Annual Conference on Neural Information Processing Systems (NIPS)</a>.<br>Abstract: <em>The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.</em><br><br>The conference will be in Montreal, Canada during December 3-8, 2018.<br>---------------------------------------------------------</li><li><strong>[01 July - 06 July, 2018]</strong><br>I am attending the&nbsp;<a href="https://ismp2018.sciencesconf.org" target="_blank" class="text-secondary">23rd International Symposium on Mathematical Programming in Bordeaux, France</a>. My talk <a href="https://ismp2018.sciencesconf.org/data/bookRoomAssingment.pdf" target="_blank" class="text-secondary">is scheduled</a> on Friday, 06 July, in the section <em>"New methods for stochastic optimization and variational inequalities".</em><br>The Talk: <em>”An Accelerated Directional Derivative Method for Smooth Stochastic Convex Optimization”</em><br>[<a href="assets/files/ismp2018_slides.pdf" target="_blank" class="text-secondary">slides</a>]<br>---------------------------------------------------------</li><li><strong>[27 June - 28 June, 2018]</strong><br>I am in Grenoble, attending the&nbsp;<a href="https://ljk.imag.fr/membres/Jerome.Malick/Oday18.html" target="_blank" class="text-secondary">Grenoble Optimization Days 2018</a>. By the way I have the new most favorite photo!&nbsp;</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-qYQqPqFyLP" id="image1-14">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 36%;">
                <img src="assets/images/favorite-photo-720x960.jpg" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-qYPQtreyf2" id="content12-w">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><font color="#ffffff">Here I read&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1711/1711.00394.pdf" target="_blank" class="text-secondary">the excellent book</a>&nbsp;by my advisor&nbsp;<a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>------------------------------------------------------------</font></div><ul>
                    <li><strong>[25 June, 2018]</strong><br>Today I successfully defended my Bachelor diploma.<br>Thesis: <a href="assets/files/gorbunov_diplom.pdf" target="_blank" class="text-secondary">"Accelerated Directional Searchs and Gradient-Free Methods with non-Euclidean prox-structure"</a><br>---------------------------------------------------------</li><li><strong>[10 June - 15 June, 2018]</strong><br>I am in&nbsp;Traditional Youth School ”Control, Information and Optimization” (Voronovo, Russia) organized by <a href="https://scholar.google.ru/citations?user=Zhlib28AAAAJ&hl=ru" target="_blank" class="text-secondary">Boris Polyak</a> and <a href="https://scholar.google.ru/citations?user=XzrS6V8AAAAJ&hl=ru" target="_blank" class="text-secondary">Elena Gryazina</a>.
 <br><br>I presented a <a href="assets/files/gorbunov_poster_tms_2018.pdf" class="text-secondary" target="_blank">poster</a> <em>"An Accelerated Directional Derivative Method for SmoothStochastic Convex Optimization". </em>My work was chosen and I gave a talk there.  I won third prize for this talk in competitions of best talks among participants. <br>[<a href="assets/files/gorbunov_slides_tms.pdf" target="_blank" class="text-secondary">slides of the talk</a>]<br><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[14 April, 2018]</strong><br>I am attending workshop <em>”Optimization at Work”</em> at MIPT (Moscow, Russia) with a talk <em>”An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization”</em> .
<br><span style="font-size: 1.5rem;">[<a href="assets/files/Optimization_at_work_2018_spring.pdf" target="_blank" class="text-secondary">slides</a>] [<a href="https://www.youtube.com/watch?v=YtWBDVZZNDk&t=291s" target="_blank" class="text-success">video</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[10 April, 2018]</strong><br><strong>New paper out: </strong><a href="https://arxiv.org/pdf/1804.03722.pdf" target="_blank" class="text-secondary">"On the upper bound for the mathematical expectation of the norm of a vector uniformly distributed on the sphere and the phenomenon of concentration of uniform measure on the sphere"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=KConUy8AAAAJ&hl=ru" target="_blank" class="text-secondary">Evgeniya Vorontsova</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>Abstract: <em>We considered the problem of obtaining upper bounds for the mathematical expectation of the q-norm (2⩽q⩽∞) of the vector which is uniformly distributed on the unit Euclidean sphere.</em><br>---------------------------------------------------------</li><li><strong>[8 April, 2018]<br>New paper out: <a href="https://arxiv.org/pdf/1804.02394.pdf" target="_blank" class="text-secondary">"</a></strong><a href="https://arxiv.org/pdf/1804.02394.pdf" target="_blank" class="text-secondary">An Accelerated Directional Derivative Method for Smooth Stochastic Convex Optimization"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=ru" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>Abstract: <em>We consider smooth stochastic convex optimization problems in the context of algorithms which are based on directional derivatives of the objective function. This context can be considered as an intermediate one between derivative-free optimization and gradient-based optimization. We assume that at any given point and for any given direction, a stochastic approximation for the directional derivative of the objective function at this point and in this direction is available with some additive noise. The noise is assumed to be of an unknown nature, but bounded in the absolute value. We underline that we consider directional derivatives in any direction, as opposed to coordinate descent methods which use only derivatives in coordinate directions. For this setting, we propose a non-accelerated and an accelerated directional derivative method and provide their complexity bounds. Despite that our algorithms do not use gradient information, our non-accelerated algorithm has a complexity bound which is, up to a factor logarithmic in problem dimension, similar to the complexity bound of gradient-based algorithms. Our accelerated algorithm has a complexity bound which coincides with the complexity bound of the accelerated gradient-based algorithm up to a factor of square root of the problem dimension, whereas for existing directional derivative methods this factor is of the order of problem dimension. We also extend these results to strongly convex problems. Finally, we consider derivative-free optimization as a particular case of directional derivative optimization with noise in the directional derivative and obtain complexity bounds for non-accelerated and accelerated derivative-free methods. Complexity bounds for these algorithms inherit the gain in the dimension dependent factors from our directional derivative methods.</em><br>---------------------------------------------------------</li><li><strong>[25 February, 2018]<br></strong><strong>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/pdf/1802.09022.pdf" target="_blank" class="text-secondary">"An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization"</a> - joint work with <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=ru" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br></span><span style="font-size: 1.5rem;">Abstract:<em> We consider an unconstrained problem of minimization of a smooth convex function which is only available through noisy observations of its values, the noise consisting of two parts. Similar to stochastic optimization problems, the first part is of a stochastic nature. On the opposite, the second part is an additive noise of an unknown nature, but bounded in the absolute value. In the two-point feedback setting, i.e. when pairs of function values are available, we propose an accelerated derivative-free algorithm together with its complexity analysis. The complexity bound of our derivative-free algorithm is only by a factor of \sqrt{n} larger than the bound for accelerated gradient-based algorithms, where n is the dimension of the decision variable. We also propose a non-accelerated derivative-free algorithm with a complexity bound similar to the stochastic-gradient-based algorithm, that is, our bound does not have any dimension-dependent factor. Interestingly, if the solution of the problem is sparse, for both our algorithms, we obtain better complexity bound if the algorithm uses a 1-norm proximal setup, rather than the Euclidean proximal setup, which is a standard choice for unconstrained problems.</em><br>---------------------------------------------------------</span></li><li><strong>[10 February, 2018]</strong><br><strong>New paper out</strong>: &nbsp;<a href="https://arxiv.org/pdf/1802.03703.pdf" target="_blank" class="text-secondary">"Stochastic spectral and conjugate descent methods"</a> - joint work with Dmitry Kovalev, Elnur Gasanov and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>Abstract: <em>The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.</em><br>---------------------------------------------------------</li><li><strong>[9 February, 2018]</strong><br>I started my final semester as a Bachelor student today. This spring I will be a <a href="teaching.html#header5-11" class="text-secondary">tutor</a>&nbsp;for the course "Algorithms and Models of Computation".<br>---------------------------------------------------------</li><li><strong>[5 February - 7 February, 2018]</strong><br>I am attending <a href="https://obd.kaust.edu.sa" target="_blank" class="text-secondary">KAUST Research Workshop on Optimization and Big Data</a> with <a href="assets/files/Optimization_and_Big_Data_2018__poster.pdf" target="_blank" class="text-secondary">poster</a> <em>”Stochastic Spectral Descent Methods” </em>based on our joint work with Dmitry Kovalev, Elnur Gasanov and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>---------------------------------------------------------</li><li><strong>[14 January - 08 February, 2018]</strong><br>I am visiting <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;at <a href="https://vcc.kaust.edu.sa/Pages/Home.aspx" target="_blank" class="text-secondary">KAUST</a>.<br>---------------------------------------------------------</li><li><strong>[25 November, 2017]<br></strong>Today I got a prize for the best talk in Section of Information Transmission Problems, Data Analysis and Optimization, 60th Scientific Conference of MIPT.<br>The talk: <em>”About accelerated Directional Search with non-Euclidean prox-structure”</em><br>[<a href="assets/files/gorbunov_60_conf_MIPT_slides.pdf" target="_blank" class="text-secondary">slides</a>]<br>---------------------------------------------------------</li><li><strong>[27 October, 2017]&nbsp;</strong><br>I am attending workshop ”Optimization at Work” at MIPT (Moscow, Russia) with a talk <em>”Accelerated Directional Search with non-Euclidean prox-structure”.<br></em><span style="font-size: 1.5rem;">[<a href="assets/files/Accelerated_Directional_Search_with_non_euclidean_prox_structure.pdf" target="_blank" class="text-secondary">slides</a>]<br><em>---------------------------------------------------------</em></span></li><li><strong>[30 September, 2017]</strong><br><strong>New paper out: </strong><a href="https://arxiv.org/pdf/1710.00162.pdf" target="_blank" class="text-secondary">"Accelerated Directional Search with non-Euclidean prox-structure"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=KConUy8AAAAJ&hl=ru" target="_blank" class="text-secondary">Evgeniya Vorontsova</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>Abstract: <em>In the paper we propose an accelerated directional search method with non-euclidian prox-structure. We consider convex unconstrained optimization problem in R^n. For simplicity we start from the zero point. We expect in advance that 1-norm of the solution is close enough to its 2-norm. In this case the standard accelerated Nesterov's directional search method can be improved. In the paper we show how to make Nesterov's method n-times faster (up to a log n-factor) in this case. The basic idea is to use linear coupling, proposed by Allen-Zhu &amp; Orecchia in 2014, and to make Grad-step in 2-norm, but Mirr-step in 1-norm. We show that for constrained optimization problem this approach stable upon an obstacle.</em><br>---------------------------------------------------------<br></li><li><strong>[29 July, 2017]&nbsp;<br></strong>I am happy to announce that I started working with <a href="https://richtarik.org" target="_blank" class="text-secondary" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">Peter Richtárik</a><span style="font-size: 1.5rem;">&nbsp;as a member of his research group at MIPT.&nbsp;</span></li>
                </ul>
            </div>
        </div>
    </div>
</section><section style="background-color: #fff; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif; color:#aaa; font-size:12px; padding: 0; align-items: center; display: flex;"><a href="https://mobirise.site/s" style="flex: 1 1; height: 3rem; padding-left: 1rem;"></a><p style="flex: 0 0 auto; margin:0; padding-right:1rem;"><a href="https://mobirise.site/r" style="color:#aaa;">The website</a> was created with Mobirise themes</p></section><script src="assets/web/assets/jquery/jquery.min.js"></script>  <script src="assets/popper/popper.min.js"></script>  <script src="assets/bootstrap/js/bootstrap.min.js"></script>  <script src="assets/tether/tether.min.js"></script>  <script src="assets/smoothscroll/smooth-scroll.js"></script>  <script src="assets/dropdown/js/nav-dropdown.js"></script>  <script src="assets/dropdown/js/navbar-dropdown.js"></script>  <script src="assets/touchswipe/jquery.touch-swipe.min.js"></script>  <script src="assets/theme/js/script.js"></script>  
  
  
</body>
</html>