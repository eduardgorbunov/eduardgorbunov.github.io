<!DOCTYPE html>
<html >
<head>
  <!-- Site made with Mobirise Website Builder v4.8.1, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v4.8.1, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/soccer-ball-128x128.png" type="image/x-icon">
  <meta name="description" content="">
  <title>Eduard Gorbunov</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
</head>
<body>
  <section class="menu cid-qTkzRZLJNu" once="menu" id="menu1-0">

    

    <nav class="navbar navbar-expand beta-menu navbar-dropdown align-items-center navbar-fixed-top navbar-toggleable-sm">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
                <span></span>
            </div>
        </button>
        <div class="menu-logo">
            <div class="navbar-brand">
                
                <span class="navbar-caption-wrap"><a class="navbar-caption text-white display-2" href="index.html">Eduard Gorbunov</a></span>
            </div>
        </div>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item">
                    <a class="nav-link link text-white display-4" href="index.html">
                        <span class="mbri-home mbr-iconfont mbr-iconfont-btn"></span>
                        Home<br></a>
                </li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#header5-16"><span class="mbri-magic-stick mbr-iconfont mbr-iconfont-btn"></span>Skills and Interests<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#timeline1-f"><span class="mbri-rocket mbr-iconfont mbr-iconfont-btn"></span>
                        
                        Education<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="index.html#header5-v"><span class="mbri-paper-plane mbr-iconfont mbr-iconfont-btn"></span>
                        
                        News<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="publications.html"><span class="mbri-file mbr-iconfont mbr-iconfont-btn"></span>
                        
                        Publications<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="conferences.html#header5-s"><span class="mbri-chat mbr-iconfont mbr-iconfont-btn"></span>
                        
                        Conference Talks<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="conferences.html#header5-t"><span class="mbri-photo mbr-iconfont mbr-iconfont-btn"></span>Posters<br></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="teaching.html"><span class="mbri-growing-chart mbr-iconfont mbr-iconfont-btn"></span>Teaching<br></a></li></ul>
            
        </div>
    </nav>
</section>

<section class="engine"><a href="https://mobiri.se/c">website builder</a></section><section class="header3 cid-qYP1dtKorN" id="header3-a">

    

    

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-figure" style="width: 30%;">
                <img src="assets/images/picture-188x334.jpg" alt="Mobirise" title="">
            </div>

            <div class="media-content">
                <h1 class="mbr-section-title mbr-white pb-3 mbr-fonts-style display-1">
                    Biography
                </h1>
                
                <div class="mbr-section-text mbr-white pb-3 ">
                    <p class="mbr-text mbr-fonts-style display-5">
                        I am a first year student of MS program at &nbsp;<a href="https://mipt.ru/english/" target="_blank">Moscow Institute of Physics and Technology</a>, <a href="https://mipt.ru/english/edu/phystechschools/psami" target="_blank">Department of Control and Applied Mathematics</a>. I work under supervision of professor <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-primary">Alexander Gasnikov</a>. My research interests include Stochastic Optimization and its applications to Machine Learning.<br><a href="https://www.researchgate.net/profile/Eduard_Gorbunov" target="_blank">ResearchGate</a><br><a href="https://arxiv.org/search/math?searchtype=author&query=Gorbunov%2C+E" target="_blank">arXiv</a><br> <a href="https://scholar.google.ru/citations?hl=ru&user=85j2RqQAAAAJ&view_op=list_works&gmla=AJsN-F4-YYAzVQc6XKMvhRw7P7WTCCxaxHeAWsMY8Ae4IxoN39CnXsfs27kTysMXKRJsoN4GMursvIkTdG0I4mhHN-6Zuq8XmsCqKI5mMC5XwvWVJBTHvbCTYwuHWdqjqfSV3db4aOq-" target="_blank">Google scholar</a><br><a href="assets/files/CV.pdf" target="_blank">Download my CV</a><br>My e-mail: ed-gorbunov at yandex dot ru<a href="assets/files/CV.pdf" target="_blank"><br></a><br>
                    </p>
                </div>
                
            </div>
        </div>
    </div>

</section>

<section class="header5 cid-qYQDzdxwR8" id="header5-16">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1">
                    Skills and Interests</h1>
                
                
            </div>
        </div>
    </div>

    
</section>

<section class="mbr-section article content12 cid-qYQEjWGZkg" id="content12-18">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><font color="#000000"><strong>Computer skills:</strong></font></div><ul>
                    <li>Operating systems: Microsoft Windows, Linux, Mac OSX</li><li>Programming languages: Python, LaTeX, C, C++<br></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-qYQFYKn1lc" id="content12-19">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><strong>Languages:</strong></div><ul>
                    <li>Russian (native)</li><li>English (advanced)</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-qYQGkPjvpj" id="content12-1a">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><font color="#000000"><strong>Interests:</strong></font></div><ul>
                    <li>Football: 9 years in football school in Rybinsk, Russia. Now I am playing for an&nbsp;<a href="http://www.lfl.ru/person86180" target="_blank" class="text-secondary">amateur team</a>&nbsp;and a&nbsp;<a href="http://mipt.nagradion.ru/tournament10438/player/51747" target="_blank" class="text-secondary">student team</a>.</li><li>Table tennis, fitness</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="timeline1 cid-qYP2SIf7dk" id="timeline1-f">

    

    

    <div class="container align-center">
        <h2 class="mbr-section-title pb-3 mbr-fonts-style display-1">
            Education</h2>
        

        <div class="container timelines-container" mbri-timelines="">
            <div class="row timeline-element reverse separline">      
                 <div class="timeline-date-panel col-xs-12 col-md-6  align-left">         
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">Sep 2014 - Jun 2018</p>
                    </div>
                </div>
           <span class="iconBackground"></span>
            <div class="col-xs-12 col-md-6 align-right">
                <div class="timeline-text-content">
                    <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">BSc at Moscow Institute of Physics and Technology</h4>
                    <p class="mbr-timeline-text mbr-fonts-style display-7">Department of Control and Applied Mathematics<br>Thesis: <a href="assets/files/gorbunov_diplom.pdf" target="_blank" class="text-secondary">Accelerated Directional Searchs and Gradient-Free Methods with non-Euclidean prox-structure</a></p>
                 </div>
            </div>
            </div>

            <div class="row timeline-element ">
                <div class="timeline-date-panel col-xs-12 col-md-6 align-right">
                    <div class="time-line-date-content">
                        <p class="mbr-timeline-date mbr-fonts-style display-5">
                            Sep 2018 - Present</p>
                    </div>
                </div>
                <span class="iconBackground"></span>
                <div class="col-xs-12 col-md-6 align-left ">
                    <div class="timeline-text-content">
                        <h4 class="mbr-timeline-title pb-3 mbr-fonts-style display-5">
                            MSc at Moscow Institute of Physics and Technology</h4>
                        <p class="mbr-timeline-text mbr-fonts-style display-7">
                            Department of Control and Applied Mathematics</p>
                    </div>
                </div>
            </div> 


            

            


            


            

            

            

            

            

            

            
        </div>
    </div>
</section>

<section class="header5 cid-qYPQ5TK196" id="header5-v">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1">
                    News</h1>
                
                
            </div>
        </div>
    </div>

    
</section>

<section class="mbr-section article content12 cid-qYQrUl0BTQ" id="content12-15">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <ul>
                    <li><strong></strong></li><li><strong>[27 May, 2019]
<br></strong><strong>New paper out: <a href="assets/files/sigma_k.pdf" target="_blank" class="text-secondary">"A Unified Theory of SGD: Variance Reduction,&nbsp;</a></strong><strong><a href="assets/files/sigma_k.pdf" target="_blank" class="text-secondary">Sampling, Quantization and Coordinate Descent"</a>&nbsp;</strong><span style="font-size: 1.5rem;">- joint work with <a href="https://fhanzely.github.io/index.html" target="_blank" class="text-secondary">Filip Hanzely</a> and&nbsp;</span><span style="font-size: 1.5rem;"><a href="https://richtarik.org/index.html" target="_blank" class="text-secondary">Peter Richtárik</a>.<br><strong>Abstract: </strong><em>In this paper we introduce a unified analysis of a large family of variants of proximal stochastic gradient descent (SGD) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling.  As a by-product, we obtain the first unified theory of SGD and randomized coordinate descent (RCD) methods,  the first unified theory of variance reduced and non-variance-reduced SGD methods, and the first unified theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop five new variants of SGD, and through numerical experiments demonstrate some of their properties.</em><br>---------------------------------------------------------<br></span></li><li><strong>[10 April, 2019]<br>I got the Ilya Segalovich Award - Yandex scientific scholarship! </strong>The award includes scholarship (total amount - 350,000 Russian rubles), insternship offer at Yandex.Research, travel grant to attend one international conference and personal mentor from Yandex.<br>[<a href="https://www.youtube.com/embed/6qkme8n3UI8" target="_blank" class="text-success">video</a>] [<a href="https://yandex.ru/scholarships/scholars" target="_blank">about award</a>] [<a href="https://habr.com/ru/company/yandex/blog/447708/" target="_blank" class="text-secondary">about winners</a>]<br>---------------------------------------------------------</li><li><strong>[23 March, 2019]<br>New paper out: <a href="https://arxiv.org/pdf/1903.09844.pdf" target="_blank" class="text-secondary">"On Dual Approach for Distributed Stochastic Convex Optimization over Networks"</a>&nbsp;- </strong>joint work with <a href="https://scholar.google.ru/citations?user=5ILnTRsAAAAJ&hl=ru" target="_blank" class="text-secondary">Darina Dvinskikh</a>, <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>, <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=ru" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://cauribe.mit.edu" target="_blank" class="text-secondary">Cesar A. Uribe</a>.<br>Abstract: <em>We introduce dual stochastic gradient oracle methods for distributed stochastic convex optimization problems over networks. We estimate the complexity of the proposed method in terms of probability of large deviations. This analysis is based on a new technique that allows to bound the distance between the iteration sequence and the solution point. By the proper choice of batch size, we can guarantee that this distance equals (up to a constant) to the distance between the starting point and the solution.<br>---------------------------------------------------------</em></li><li><strong>[10 February, 2019]
<br></strong><strong>New paper out:  </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/pdf/1902.03591.pdf" target="_blank" class="text-secondary">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a> - joint work with &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<a href="http://maiage.jouy.inra.fr/?q=fr/bergou" target="_blank" class="text-secondary">El Houcine Bergou</a> and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.&nbsp;<br></span><span style="font-size: 1.5rem;">Abstract: <em>In this paper we consider the unconstrained minimization problem of a smooth function in R^n in a setting where only function evaluations are possible. We design a novel randomized derivative-free algorithm --- the stochastic three points (STP) method --- and analyze its iteration complexity. At each iteration, STP generates a random search direction according to a certain fixed probability law. Our assumptions on this law are very mild: roughly speaking, all laws which do not concentrate all measure on any halfspace passing through the origin will work. For instance, we allow for the uniform distribution on the sphere and also distributions that concentrate all measure on a positive spanning set.
<br></em></span><em>Given a current iterate x, STP compares the objective function at three points: x, x+αs and x−αs, where α&gt;0 is a stepsize parameter and s is the random search direction. The best of these three points is the next iterate. We analyze the method STP under several stepsize selection schemes (fixed, decreasing, estimated through finite differences, etc). We study non-convex, convex and strongly convex cases.&nbsp;<br></em><em>We also propose a parallel version for STP, with iteration complexity bounds which do not depend on the dimension n.&nbsp;<br></em>arXiv: <a href="https://arxiv.org/abs/1902.03591" target="_blank" class="text-secondary" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">1901.09269</a><span style="font-size: 1.5rem;">&nbsp;<br></span>---------------------------------------------------------</li><li><strong>[08 February, 2019]</strong><br>I am a reviewer for <a href="https://www.icml.cc" target="_blank" class="text-secondary">ICML 2019</a>! This is my first time being a reviewer for such a conference.<br>---------------------------------------------------------</li><li><strong>[26 January, 2019]<br>New paper out: &nbsp;</strong><a href="assets/files/DIANA.pdf" target="_blank" class="text-secondary">"Distributed learning with compressed gradient differences"</a> - joint work with <a href="https://konstmish.github.io" target="_blank" class="text-secondary">Konstantin Mishchenko</a>, <a href="http://mtakac.com" target="_blank" class="text-secondary">Martin Takáč</a> and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>Abstract: <em>Training very large machine learning models requires a distributed computing approach, with communication of the model updates often being the bottleneck. For this reason, several methods based on the compression (e.g., sparsification and/or quantization) of the updates were recently proposed, including QSGD (Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods are able to learn the gradients, which means that they necessarily suffer from several issues, such as the inability to converge to the true optimum in the batch mode, inability to work with a nonsmooth regularizer, and slow convergence rates. In this work we propose a new distributed learning method - DIANA - which resolves these issues via compression of gradient differences. We perform a theoretical analysis in the strongly convex and nonconvex settings and show that our rates are vastly superior to existing rates. Our analysis of block quantization and differences between l2 and l∞ quantization closes the gaps in theory and practice. Finally, by applying our analysis technique to TernGrad, we establish the first convergence rate for this method.</em><br>arXiv: <a href="https://arxiv.org/abs/1901.09269" target="_blank" class="text-secondary">1901.09269</a><br>---------------------------------------------------------</li><li><strong>[13 January - 24 February, 2019]
<br></strong><span style="font-size: 1.5rem;">I am visiting <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a> at <a href="https://vcc.kaust.edu.sa/Pages/Home.aspx" target="_blank" class="text-secondary">KAUST</a>.&nbsp;<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------&nbsp;</span></li><li><strong>[05 September, 2018]</strong><br>The paper "<a href="https://arxiv.org/abs/1802.03703" target="_blank" class="text-secondary">Stochastic spectral and conjugate descent methods</a>" - joint work with Dmitry Kovalev, Elnur Gasanov and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a> &nbsp;- accepted to <a href="https://nips.cc" target="_blank" class="text-secondary">The Thirty-second Annual Conference on Neural Information Processing Systems (NIPS)</a>.<br>Abstract: <em>The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.</em><br><br>The conference will be in Montreal, Canada during December 3-8, 2018.<br>---------------------------------------------------------</li><li><strong>[01 July - 06 July, 2018]</strong><br>I am attending the&nbsp;<a href="https://ismp2018.sciencesconf.org" target="_blank" class="text-secondary">23rd International Symposium on Mathematical Programming in Bordeaux, France</a>. My talk <a href="https://ismp2018.sciencesconf.org/data/bookRoomAssingment.pdf" target="_blank" class="text-secondary">is scheduled</a> on Friday, 06 July, in the section <em>"New methods for stochastic optimization and variational inequalities".</em><br>The Talk: <em>”An Accelerated Directional Derivative Method for Smooth Stochastic Convex Optimization”</em><br>[<a href="assets/files/ismp2018_slides.pdf" target="_blank" class="text-secondary">slides</a>]<br>---------------------------------------------------------</li><li><strong>[27 June - 28 June, 2018]</strong><br>I am in Grenoble, attending the&nbsp;<a href="https://ljk.imag.fr/membres/Jerome.Malick/Oday18.html" target="_blank" class="text-secondary">Grenoble Optimization Days 2018</a>. By the way I have the new most favorite photo!&nbsp;</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section class="cid-qYQqPqFyLP" id="image1-14">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 36%;">
                <img src="assets/images/favorite-photo-720x960.jpg" width="1400" alt="Mobirise" title="">
                
            </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-qYPQtreyf2" id="content12-w">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style display-5">
                <div><font color="#ffffff">Here I read&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1711/1711.00394.pdf" target="_blank" class="text-secondary">the excellent book</a>&nbsp;by my advisor&nbsp;<a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>------------------------------------------------------------</font></div><ul>
                    <li><strong>[25 June, 2018]</strong><br>Today I successfully defended my Bachelor diploma.<br>Thesis: <a href="assets/files/gorbunov_diplom.pdf" target="_blank" class="text-secondary">"Accelerated Directional Searchs and Gradient-Free Methods with non-Euclidean prox-structure"</a><br>---------------------------------------------------------</li><li><strong>[10 June - 15 June, 2018]</strong><br>I am in&nbsp;Traditional Youth School ”Control, Information and Optimization” (Voronovo, Russia) organized by <a href="https://scholar.google.ru/citations?user=Zhlib28AAAAJ&hl=ru" target="_blank" class="text-secondary">Boris Polyak</a> and <a href="https://scholar.google.ru/citations?user=XzrS6V8AAAAJ&hl=ru" target="_blank" class="text-secondary">Elena Gryazina</a>.
 <br><br>I presented a <a href="assets/files/gorbunov_poster_tms_2018.pdf" class="text-secondary" target="_blank">poster</a> <em>"An Accelerated Directional Derivative Method for SmoothStochastic Convex Optimization". </em>My work was chosen and I gave a talk there.  I won third prize for this talk in competitions of best talks among participants. <br>[<a href="assets/files/gorbunov_slides_tms.pdf" target="_blank" class="text-secondary">slides of the talk</a>]<br><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[14 April, 2018]</strong><br>I am attending workshop <em>”Optimization at Work”</em> at MIPT (Moscow, Russia) with a talk <em>”An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization”</em> .
<br><span style="font-size: 1.5rem;">[<a href="assets/files/Optimization_at_work_2018_spring.pdf" target="_blank" class="text-secondary">slides</a>] [<a href="https://www.youtube.com/watch?v=YtWBDVZZNDk&t=291s" target="_blank" class="text-success">video</a>]<br></span><span style="font-size: 1.5rem;">---------------------------------------------------------</span></li><li><strong>[10 April, 2018]</strong><br><strong>New paper out: </strong><a href="https://arxiv.org/pdf/1804.03722.pdf" target="_blank" class="text-secondary">"On the upper bound for the mathematical expectation of the norm of a vector uniformly distributed on the sphere and the phenomenon of concentration of uniform measure on the sphere"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=KConUy8AAAAJ&hl=ru" target="_blank" class="text-secondary">Evgeniya Vorontsova</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>Abstract: <em>We considered the problem of obtaining upper bounds for the mathematical expectation of the q-norm (2⩽q⩽∞) of the vector which is uniformly distributed on the unit Euclidean sphere.</em><br>---------------------------------------------------------</li><li><strong>[8 April, 2018]<br>New paper out: <a href="https://arxiv.org/pdf/1804.02394.pdf" target="_blank" class="text-secondary">"</a></strong><a href="https://arxiv.org/pdf/1804.02394.pdf" target="_blank" class="text-secondary">An Accelerated Directional Derivative Method for Smooth Stochastic Convex Optimization"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=ru" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>Abstract: <em>We consider smooth stochastic convex optimization problems in the context of algorithms which are based on directional derivatives of the objective function. This context can be considered as an intermediate one between derivative-free optimization and gradient-based optimization. We assume that at any given point and for any given direction, a stochastic approximation for the directional derivative of the objective function at this point and in this direction is available with some additive noise. The noise is assumed to be of an unknown nature, but bounded in the absolute value. We underline that we consider directional derivatives in any direction, as opposed to coordinate descent methods which use only derivatives in coordinate directions. For this setting, we propose a non-accelerated and an accelerated directional derivative method and provide their complexity bounds. Despite that our algorithms do not use gradient information, our non-accelerated algorithm has a complexity bound which is, up to a factor logarithmic in problem dimension, similar to the complexity bound of gradient-based algorithms. Our accelerated algorithm has a complexity bound which coincides with the complexity bound of the accelerated gradient-based algorithm up to a factor of square root of the problem dimension, whereas for existing directional derivative methods this factor is of the order of problem dimension. We also extend these results to strongly convex problems. Finally, we consider derivative-free optimization as a particular case of directional derivative optimization with noise in the directional derivative and obtain complexity bounds for non-accelerated and accelerated derivative-free methods. Complexity bounds for these algorithms inherit the gain in the dimension dependent factors from our directional derivative methods.</em><br>---------------------------------------------------------</li><li><strong>[25 February, 2018]<br></strong><strong>New paper out: </strong><span style="font-size: 1.5rem;"><a href="https://arxiv.org/pdf/1802.09022.pdf" target="_blank" class="text-secondary">"An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization"</a> - joint work with <a href="https://scholar.google.ru/citations?user=28MSou8AAAAJ&hl=ru" target="_blank" class="text-secondary">Pavel Dvurechensky</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br></span><span style="font-size: 1.5rem;">Abstract:<em> We consider an unconstrained problem of minimization of a smooth convex function which is only available through noisy observations of its values, the noise consisting of two parts. Similar to stochastic optimization problems, the first part is of a stochastic nature. On the opposite, the second part is an additive noise of an unknown nature, but bounded in the absolute value. In the two-point feedback setting, i.e. when pairs of function values are available, we propose an accelerated derivative-free algorithm together with its complexity analysis. The complexity bound of our derivative-free algorithm is only by a factor of \sqrt{n} larger than the bound for accelerated gradient-based algorithms, where n is the dimension of the decision variable. We also propose a non-accelerated derivative-free algorithm with a complexity bound similar to the stochastic-gradient-based algorithm, that is, our bound does not have any dimension-dependent factor. Interestingly, if the solution of the problem is sparse, for both our algorithms, we obtain better complexity bound if the algorithm uses a 1-norm proximal setup, rather than the Euclidean proximal setup, which is a standard choice for unconstrained problems.</em><br>---------------------------------------------------------</span></li><li><strong>[10 February, 2018]</strong><br><strong>New paper out</strong>: &nbsp;<a href="https://arxiv.org/pdf/1802.03703.pdf" target="_blank" class="text-secondary">"Stochastic spectral and conjugate descent methods"</a> - joint work with Dmitry Kovalev, Elnur Gasanov and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>Abstract: <em>The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.</em><br>---------------------------------------------------------</li><li><strong>[9 February, 2018]</strong><br>I started my final semester as a Bachelor student today. This spring I will be a <a href="teaching.html#header5-11" class="text-secondary">tutor</a>&nbsp;for the course "Algorithms and Models of Computation".<br>---------------------------------------------------------</li><li><strong>[5 February - 7 February, 2018]</strong><br>I am attending <a href="https://obd.kaust.edu.sa" target="_blank" class="text-secondary">KAUST Research Workshop on Optimization and Big Data</a> with <a href="assets/files/Optimization_and_Big_Data_2018__poster.pdf" target="_blank" class="text-secondary">poster</a> <em>”Stochastic Spectral Descent Methods” </em>based on our joint work with Dmitry Kovalev, Elnur Gasanov and <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>.<br>---------------------------------------------------------</li><li><strong>[14 January - 08 February, 2018]</strong><br>I am visiting <a href="https://richtarik.org" target="_blank" class="text-secondary">Peter Richtárik</a>&nbsp;at <a href="https://vcc.kaust.edu.sa/Pages/Home.aspx" target="_blank" class="text-secondary">KAUST</a>.<br>---------------------------------------------------------</li><li><strong>[25 November, 2017]<br></strong>Today I got a prize for the best talk in Section of Information Transmission Problems, Data Analysis and Optimization, 60th Scientific Conference of MIPT.<br>The talk: <em>”About accelerated Directional Search with non-Euclidean prox-structure”</em><br>[<a href="assets/files/gorbunov_60_conf_MIPT_slides.pdf" target="_blank" class="text-secondary">slides</a>]<br>---------------------------------------------------------</li><li><strong>[27 October, 2017]&nbsp;</strong><br>I am attending workshop ”Optimization at Work” at MIPT (Moscow, Russia) with a talk <em>”Accelerated Directional Search with non-Euclidean prox-structure”.<br></em><span style="font-size: 1.5rem;">[<a href="assets/files/Accelerated_Directional_Search_with_non_euclidean_prox_structure.pdf" target="_blank" class="text-secondary">slides</a>]<br><em>---------------------------------------------------------</em></span></li><li><strong>[30 September, 2017]</strong><br><strong>New paper out: </strong><a href="https://arxiv.org/pdf/1710.00162.pdf" target="_blank" class="text-secondary">"Accelerated Directional Search with non-Euclidean prox-structure"</a>&nbsp;- joint work with <a href="https://scholar.google.ru/citations?user=KConUy8AAAAJ&hl=ru" target="_blank" class="text-secondary">Evgeniya Vorontsova</a> and <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ&hl=ru" target="_blank" class="text-secondary">Alexander Gasnikov</a>.<br>Abstract: <em>In the paper we propose an accelerated directional search method with non-euclidian prox-structure. We consider convex unconstrained optimization problem in R^n. For simplicity we start from the zero point. We expect in advance that 1-norm of the solution is close enough to its 2-norm. In this case the standard accelerated Nesterov's directional search method can be improved. In the paper we show how to make Nesterov's method n-times faster (up to a log n-factor) in this case. The basic idea is to use linear coupling, proposed by Allen-Zhu &amp; Orecchia in 2014, and to make Grad-step in 2-norm, but Mirr-step in 1-norm. We show that for constrained optimization problem this approach stable upon an obstacle.</em><br>---------------------------------------------------------<br></li><li><strong>[29 July, 2017]&nbsp;<br></strong>I am happy to announce that I started working with <a href="https://richtarik.org" target="_blank" class="text-secondary" style="font-size: 1.5rem; background-color: rgb(7, 59, 76);">Peter Richtárik</a><span style="font-size: 1.5rem;">&nbsp;as a member of his research group at MIPT.&nbsp;</span></li>
                </ul>
            </div>
        </div>
    </div>
</section>


  <script src="assets/web/assets/jquery/jquery.min.js"></script>
  <script src="assets/popper/popper.min.js"></script>
  <script src="assets/tether/tether.min.js"></script>
  <script src="assets/bootstrap/js/bootstrap.min.js"></script>
  <script src="assets/smoothscroll/smooth-scroll.js"></script>
  <script src="assets/dropdown/js/script.min.js"></script>
  <script src="assets/touchswipe/jquery.touch-swipe.min.js"></script>
  <script src="assets/theme/js/script.js"></script>
  
  
</body>
</html>